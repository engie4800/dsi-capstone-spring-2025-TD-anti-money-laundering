{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FbyXkHW3iZu"
      },
      "source": [
        "# Anti-Money Laundering\n",
        "\n",
        "A brain dump of anti-money-laundering (AML) code snippets for the Data Science capstone project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ1LnC7kosD6"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Captures the set of Python imports the Notebook requires, as well as any constants defined for the analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wllxj0A5oozy"
      },
      "outputs": [],
      "source": [
        "import calendar\n",
        "import hashlib\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from datetime import datetime\n",
        "from pprint import pprint\n",
        "from time import monotonic\n",
        "from typing import Union\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "\n",
        "content_base = \"/content/drive\"\n",
        "data_dir = os.path.join(content_base, \"My Drive/capstone/data\")\n",
        "dataset = \"LI-Small_Trans.csv\"\n",
        "data_file = os.path.join(data_dir, dataset)\n",
        "\n",
        "# Some portions of the analysis are skipped due to how costly they may\n",
        "# be, or that they only needed to be executed once\n",
        "check_dataset_uniqueness = False\n",
        "create_unique_identifiers = False\n",
        "compare_dataset_conversion_rates = False\n",
        "get_new_usd_conversion_rates = True\n",
        "plot_exchange_rate_time_series = True\n",
        "plot_exchange_rate_heat_map = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7HNlNcErKfV"
      },
      "source": [
        "### Notebook Stuff\n",
        "\n",
        "Not important to the project at all, just modifying aspects of the notebook runtime for my own use. This assumes you're running the notebook in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v53m-X9lsnce"
      },
      "outputs": [],
      "source": [
        "# Google Colaboratory executes in an environment with a file system\n",
        "# that has a Linux topography, but where the user should work under\n",
        "# the `/content` directory\n",
        "COLAB_ROOT = \"/content\"\n",
        "\n",
        "REPO_URL = \"https://github.com/engie4800/dsi-capstone-spring-2025-TD-anti-money-laundering.git\"\n",
        "REPO_ROOT = os.path.join(COLAB_ROOT, REPO_URL.split(\"/\")[-1].split(\".\")[0])\n",
        "REPO_BRANCH = \"colin\"\n",
        "\n",
        "# Clones the repository at `/content/dsi-capstone-spring-2025-TD-anti-money-laundering`\n",
        "if not os.path.exists(REPO_ROOT):\n",
        "  os.chdir(COLAB_ROOT)\n",
        "  !git clone {REPO_URL}\n",
        "\n",
        "# Pulls the latest code from the provided branch and adds the\n",
        "# analysis pipeline source code to the Python system path\n",
        "os.chdir(REPO_ROOT)\n",
        "!git pull\n",
        "!git checkout {REPO_BRANCH}\n",
        "sys.path.append(os.path.join(REPO_ROOT, \"Code/src\"))\n",
        "os.chdir(COLAB_ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helpers import add_cell_timer\n",
        "\n",
        "add_cell_timer()"
      ],
      "metadata": {
        "id": "sj4GDK_x_I-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z_Dv95-pD1O"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GTs_zNP3yM9"
      },
      "outputs": [],
      "source": [
        "drive.mount(content_base)\n",
        "files = os.listdir(data_dir)\n",
        "print(\"\\nData files available:\")\n",
        "pprint(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSCDdNRapkRE"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4tzO0ctu_J8"
      },
      "source": [
        "## Data Overview\n",
        "\n",
        "Explore aspects of the data without applying any transformations or doing any feature engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu6DDINsv-Xi"
      },
      "source": [
        "### Features\n",
        "\n",
        "The selected data set has the following features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB-G-vmYvE-H"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG_84moE1YJM"
      },
      "source": [
        "Rename the features to be more explicit with the hope of avoiding common mistakes, e.g. mistaking `Account` and `Account.1`. The new names use snake case because we're in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU9HQus-1H53"
      },
      "outputs": [],
      "source": [
        "df.rename(\n",
        "    columns={\n",
        "        \"Timestamp\": \"timestamp\",\n",
        "        \"From Bank\": \"from_bank\",\n",
        "        \"Account\": \"from_account\",\n",
        "        \"To Bank\": \"to_bank\",\n",
        "        \"Account.1\": \"to_account\",\n",
        "        \"Amount Received\": \"received_amount\",\n",
        "        \"Receiving Currency\": \"received_currency\",\n",
        "        \"Amount Paid\": \"sent_amount\",\n",
        "        \"Payment Currency\": \"sent_currency\",\n",
        "        \"Payment Format\": \"payment_type\",\n",
        "        \"Is Laundering\": \"is_laundering\",\n",
        "    },\n",
        "    inplace=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJvV02HM2gA7"
      },
      "source": [
        "### Data Description\n",
        "\n",
        "Provides a general overview of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KOFgirw2mQY"
      },
      "outputs": [],
      "source": [
        "df.select_dtypes(include=[\"number\"]).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7d6I4gM3eqa"
      },
      "outputs": [],
      "source": [
        "df.select_dtypes(include=[\"object\", \"category\"]).drop(columns=\"timestamp\").describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7EJsthv4OKT"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLSwYlilwQ_q"
      },
      "source": [
        "### Missing Values\n",
        "\n",
        "Determines whether there are missing values. There aren't any in the initial exploration of the data, and so the following cell will cause the notebook to fail if null values are present, as it may violate an assumption made by subsequent steps in the analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIoL3A70wXUi"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()\n",
        "\n",
        "if df.isnull().values.any():\n",
        "    raise ValueError(\n",
        "        \"Initial analysis showed that there were no null values in the data \"\n",
        "        \"set, and the proceeding work was done under this assumption. \"\n",
        "        \"However, null values were detected. Does the dataset now need to be \"\n",
        "        \"cleaned prior to analysis?\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transaction-Level Transform\n",
        "\n",
        "Some of the columns we will add to the dataset are simple permutations within each row. These can be added prior to any train-test splits. It might be preferable to add them earlier, so they can be included in some of the below exploratory data analysis."
      ],
      "metadata": {
        "id": "W_ollPs9CCWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Day of Week, Month\n",
        "\n",
        "Convert the timestamp into day of week and month categorical variables."
      ],
      "metadata": {
        "id": "IOBGRI4MCUVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['datetime'] = pd.to_datetime(df['timestamp'], format='%Y/%m/%d %H:%M')\n",
        "\n",
        "df['day_of_week'] = df['datetime'].dt.day_name()\n",
        "df['hour_of_day'] = df['datetime'].dt.hour.astype(str)\n",
        "df['month'] = df['datetime'].dt.month_name()\n",
        "\n",
        "df[[\n",
        "    \"timestamp\",\n",
        "    \"day_of_week\",\n",
        "    \"hour_of_day\",\n",
        "    \"month\",\n",
        "]].head()"
      ],
      "metadata": {
        "id": "jqtaQcD2CgNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mbL4tOJ-QVj"
      },
      "source": [
        "## Data Imbalance\n",
        "\n",
        "Consider the shape of the data, and explore how the base features are balanced."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Laundering Rate\n",
        "\n",
        "Most anti-money-laundering datasets have an imbalance between the number of licit and illicit transactions. For imbalanced datasets, care needs to be taken in the metric chosen to measure model performance or experimental results."
      ],
      "metadata": {
        "id": "cmnns_XQDyMu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DUE2pa89WEi"
      },
      "outputs": [],
      "source": [
        "print(f'Laundering rate: {round(100*(df[\"is_laundering\"].sum() / len(df[\"is_laundering\"])), 3)}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_4hBtXa-enB"
      },
      "source": [
        "### Categorical Imbalance\n",
        "\n",
        "The balance in categorical features can be handled directly. Numerical features need to be binned prior to demonstrating imbalance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7xSdb6J-hMz"
      },
      "outputs": [],
      "source": [
        "def plot_column_imbalance(\n",
        "    df: pd.DataFrame,\n",
        "    column: str,\n",
        "    label: str,\n",
        "    bins: list[Union[int, float]]=[0, 10, 100, 1000, 10000, np.inf],\n",
        ") -> None:\n",
        "    if pd.api.types.is_numeric_dtype(df[column]):\n",
        "        # Custom bins for numerical data\n",
        "        df[\"binned\"] = pd.cut(\n",
        "            df[column],\n",
        "            bins=bins,\n",
        "            include_lowest=True,\n",
        "        )\n",
        "        bin_labels = {\n",
        "            interval: f\"{int(interval.left)} - {int(interval.right) if interval.right != np.inf else 'âˆž'}\"\n",
        "            for interval in df[\"binned\"].cat.categories\n",
        "        }\n",
        "\n",
        "        df[\"binned\"] = df[\"binned\"].map(bin_labels)\n",
        "        all_types = sorted(\n",
        "            bin_labels.values(),\n",
        "            key=lambda x: int(x.split(\" - \")[0]),\n",
        "        )\n",
        "        data_column = \"binned\"\n",
        "    else:\n",
        "        if column == \"hour_of_day\":\n",
        "            all_types = sorted(df[column].unique(), key=int)\n",
        "        else:\n",
        "            all_types = sorted(df[column].unique())\n",
        "        data_column = column\n",
        "\n",
        "    df_licit = df[df[\"is_laundering\"] == 0]\n",
        "    proportion_licit = df_licit[data_column].value_counts(normalize=True) * 100\n",
        "    proportion_licit = proportion_licit.reindex(all_types, fill_value=0)\n",
        "\n",
        "    df_illicit = df[df[\"is_laundering\"] == 1]\n",
        "    proportion_illicit = df_illicit[data_column].value_counts(normalize=True) * 100\n",
        "    proportion_illicit = proportion_illicit.reindex(all_types, fill_value=0)\n",
        "\n",
        "    total_proportion = proportion_licit + proportion_illicit\n",
        "    licit_normalized = (proportion_licit / total_proportion) * 100\n",
        "    illicit_normalized = (proportion_illicit / total_proportion) * 100\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7, 4))\n",
        "    y_pos = np.arange(len(all_types))\n",
        "\n",
        "    ax.barh(y_pos, licit_normalized, color=\"#76c7c0\", label=\"Licit\")\n",
        "    ax.barh(y_pos, illicit_normalized, left=licit_normalized, color=\"#f4a261\", label=\"Illicit\")\n",
        "    ax.axvline(50, linestyle=\"--\", color=\"gray\", linewidth=1)\n",
        "\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(all_types)\n",
        "    ax.set_xlabel(\"Proportion\")\n",
        "    ax.set_title(f\"{label}, Licit vs. Illicit\")\n",
        "    ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "column_mapping_imbalance = {\n",
        "    \"Payment Type\": \"payment_type\",\n",
        "    \"Sent Currency\": \"sent_currency\",\n",
        "    \"Received Currency\": \"received_currency\",\n",
        "    \"Sent Amount\": \"sent_amount\",\n",
        "    \"Received Amount\": \"received_amount\",\n",
        "    \"Day of Week\": \"day_of_week\",\n",
        "    \"Hour of Day\": \"hour_of_day\",\n",
        "    \"Month\": \"month\",\n",
        "}\n",
        "dropdown_imbalance = widgets.Dropdown(\n",
        "    options=column_mapping_imbalance.keys(),\n",
        "    description=\"Column:\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        ")\n",
        "\n",
        "def update_plot(column_label: str) -> None:\n",
        "    column = column_mapping_imbalance[column_label]\n",
        "    plot_column_imbalance(df, column, column_label)\n",
        "\n",
        "widgets.interactive(update_plot, column_label=dropdown_imbalance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekcLrbiqBswK"
      },
      "source": [
        "## Bank+Account Uniqueness\n",
        "\n",
        "Between the datasets in the following subsesction, are account numbers unique? Given that they aren't, provide a method to make them unique."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intra-dataset Uniqueness\n",
        "\n",
        "The goal is to determine whether the AMLworld synthetic data generates unique account numbers per dataset, which, if unique, might give us the opportunity to train on one dataset and test on another. This is done by streaming the data files and not by loading the files into a data frame because loading the large files into memory is not possible in many computation environments. Even so, this takes a long time."
      ],
      "metadata": {
        "id": "YQjEwyoZ40qx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLSUN8p2DWIM"
      },
      "outputs": [],
      "source": [
        "def check_pairwise_dataset_uniqueness(\n",
        "    dataset_a: str,\n",
        "    dataset_b: str,\n",
        ") -> None:\n",
        "    hash_map = {}\n",
        "    hash_map_aggregate = {}\n",
        "    poor_account_uniqueness = \"Account Uniqueness header mismatch\"\n",
        "    for i, dataset in enumerate([dataset_a, dataset_b]):\n",
        "        print(f\"Hashing: {dataset}\")\n",
        "        with open(\n",
        "            os.path.join(data_dir, dataset), \"r\", encoding=\"utf-8\",\n",
        "        ) as file:\n",
        "            header = True\n",
        "            for line in file:\n",
        "                columns = line.strip().split(\",\")\n",
        "\n",
        "                # Checks that each data set is formatted with the account data\n",
        "                # in the same location\n",
        "                if header:\n",
        "                    header = False\n",
        "                    if (\n",
        "                        columns[1] != \"From Bank\" or\n",
        "                        columns[2] != \"Account\" or\n",
        "                        columns[3] != \"To Bank\" or\n",
        "                        columns[4] != \"Account\"\n",
        "                    ):\n",
        "                        raise ValueError(poor_account_uniqueness)\n",
        "                    continue\n",
        "\n",
        "                # Hash on both the from and the to account, keeping track of an\n",
        "                # enumerated dataset\n",
        "                for account in [columns[2], columns[4]]:\n",
        "                    if account not in hash_map:\n",
        "                        hash_map[account] = [i]\n",
        "                    elif i not in hash_map[account]:\n",
        "                        hash_map[account].append(i)\n",
        "\n",
        "                # Hash on a combination of bank and account, for both the from\n",
        "                # and to account/bank\n",
        "                for bank_account in [\n",
        "                    f\"{columns[1]}_{columns[2]}\",\n",
        "                    f\"{columns[3]}_{columns[4]}\",\n",
        "                ]:\n",
        "                    if bank_account not in hash_map_aggregate:\n",
        "                        hash_map_aggregate[bank_account] = [i]\n",
        "                    elif i not in hash_map_aggregate[bank_account]:\n",
        "                        hash_map_aggregate[bank_account].append(i)\n",
        "\n",
        "    # Checks for duplicate accounts\n",
        "    count = 0\n",
        "    for account, datasets in hash_map.items():\n",
        "        if len(datasets) > 1:\n",
        "            count += 1\n",
        "    n = len(hash_map)\n",
        "    print(f\"Hash map by account: {n}, duplicate accounts: {count}\")\n",
        "    print(f\"Uniqueness by account: {round(100*(n-count)/n, 3)}%\")\n",
        "\n",
        "    # Checks for duplicate account, bank pairs\n",
        "    count = 0\n",
        "    for account, datasets in hash_map_aggregate.items():\n",
        "        if len(datasets) > 1:\n",
        "            count += 1\n",
        "    n = len(hash_map_aggregate)\n",
        "    print(f\"Hash map by bank_account: {n}, duplicates: {count}\")\n",
        "    print(f\"Uniqueness by bank_account: {round(100*(n-count)/n, 3)}%\")\n",
        "\n",
        "if check_dataset_uniqueness:\n",
        "    check_pairwise_dataset_uniqueness(\n",
        "        \"LI-Medium_Trans.csv\",\n",
        "        \"LI-Small_Trans.csv\",\n",
        "    )\n",
        "    print(\"\")\n",
        "    check_pairwise_dataset_uniqueness(\n",
        "        \"HI-Medium_Trans.csv\",\n",
        "        \"LI-Medium_Trans.csv\",\n",
        "    )\n",
        "    print(\"\")\n",
        "    check_pairwise_dataset_uniqueness(\n",
        "        \"LI-Large_Trans.csv\",\n",
        "        \"LI-Medium_Trans.csv\",\n",
        "    )\n",
        "else:\n",
        "    print(\"Skipped potentially lengthy uniqueness check.\")\n",
        "\n",
        "    # Keeping a snapshot of a previous analysis\n",
        "    print(\"Data from a previous run:\")\n",
        "    print(\"\"\"\n",
        "Hashing: LI-Medium_Trans.csv\n",
        "Hashing: LI-Small_Trans.csv\n",
        "Hash map by account: 2721565, duplicate accounts: 16399\n",
        "Uniqueness by account: 99.397%\n",
        "Hash map by bank_account: 2737985, duplicates: 17\n",
        "Uniqueness by bank_account: 99.999%\n",
        "\n",
        "Hashing: HI-Medium_Trans.csv\n",
        "Hashing: LI-Medium_Trans.csv\n",
        "Hash map by account: 4047087, duplicate accounts: 61973\n",
        "Uniqueness by account: 98.469%\n",
        "Hash map by bank_account: 4094704, duplicates: 14414\n",
        "Uniqueness by bank_account: 99.648%\n",
        "\n",
        "Hashing: LI-Large_Trans.csv\n",
        "Hashing: LI-Medium_Trans.csv\n",
        "Hash map by account: 2054565, duplicate accounts: 2031886\n",
        "Uniqueness by account: 1.104%\n",
        "Hash map by bank_account: 2071157, duplicates: 2031918\n",
        "Uniqueness by bank_account: 1.895%\n",
        "    \"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Uniqueness\n",
        "\n",
        "For the given data set name, and for each bank and account number, create a unique identifier. This will ensure that if models are trained on one dataset, they can be transferred to or tested on other datasets without worrying that the model learned identifiers that happen to be non-distinct between the AMLworld datasets.\n",
        "\n",
        "If the following is used to create unique entity identifiers between datasets, it needs to be applied to two datasets and tested (applying it to one of the larger datasets will be computationally intensive)."
      ],
      "metadata": {
        "id": "YG0_jS_p5NBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def h(value: str, length=8):\n",
        "    return hashlib.sha256(value.encode()).hexdigest()[:length]\n",
        "\n",
        "def generate_unique_identifiers(dataset_name, df):\n",
        "    d = h(dataset_name)\n",
        "\n",
        "    df[\"from_bank_hash\"] = df[\"from_bank\"].astype(str).map(h)\n",
        "    df[\"from_account_hash\"] = df[\"from_account\"].astype(str).map(h)\n",
        "    df[\"to_bank_hash\"] = df[\"to_bank\"].astype(str).map(h)\n",
        "    df[\"to_account_hash\"] = df[\"to_account\"].astype(str).map(h)\n",
        "\n",
        "    df[\"from_unique\"] = d + \"_\" + df[\"from_bank_hash\"] + \"_\" + df[\"from_account_hash\"]\n",
        "    df[\"to_unique\"] = d + \"_\" + df[\"to_bank_hash\"] + \"_\" + df[\"to_account_hash\"]\n",
        "\n",
        "    # Drop intermediate hash columns\n",
        "    df.drop(\n",
        "        columns=[\n",
        "            \"from_bank_hash\",\n",
        "            \"from_account_hash\",\n",
        "            \"to_bank_hash\",\n",
        "            \"to_account_hash\",\n",
        "        ],\n",
        "        inplace=True,\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "if create_unique_identifiers:\n",
        "    dataset_name = data_file.split(\"/\")[-1]\n",
        "    df = generate_unique_identifiers(dataset_name, df)\n",
        "    df[[\n",
        "        \"from_bank\",\n",
        "        \"from_account\",\n",
        "        \"from_unique\",\n",
        "        \"to_bank\",\n",
        "        \"to_account\",\n",
        "        \"to_unique\",\n",
        "    ]].head()\n",
        "else:\n",
        "    print(\"Skipped creating unique identifiers.\")"
      ],
      "metadata": {
        "id": "6cOeyPme6Emt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customer Data\n",
        "\n",
        "The dataset consists of a series of timestamped transactions with a sending and receiving customer. This section explores the length (in time) of the dataset and at the distribution of data available per customer."
      ],
      "metadata": {
        "id": "oVuwZao6EcKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Currency Conversion\n",
        "\n",
        "The given transaction dataset contains different currencies. To convert between these currencies, e.g. to convert every transaction into U.S. dollars, the currency conversion rates need to be derived from the dataset (live currency conversions will not apply to the dataset)."
      ],
      "metadata": {
        "id": "T0PPVI6gCfMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_usd_conversion(\n",
        "    currency_conversion: dict[str, dict[str, float]],\n",
        ") -> dict[str, float]:\n",
        "    \"\"\"\n",
        "    Given the result of `get_usd_conversion`, that is a dictionary that maps\n",
        "    each currency to its currency conversion rates, where each set of\n",
        "    conversion rates may be incomplete, attempt to return a complete set\n",
        "    of currency conversion rates for U.S. dollars\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\n",
        "        \"This 'extract_usd_conversion' has not been implemented, because the \"\n",
        "        \"data from the small transaction set provides a complete set of \"\n",
        "        \"U.S. dollar conversion rates.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def exchange_rate_time_series(df: pd.DataFrame, ylabel: str) -> None:\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    i = 0\n",
        "    cmap = plt.get_cmap(\"tab20\")\n",
        "    for currency in df.columns:\n",
        "        color = cmap(i)\n",
        "        plt.plot(\n",
        "            df.index,\n",
        "            df[currency],\n",
        "            marker=\".\",\n",
        "            markersize=8,\n",
        "            label=currency,\n",
        "            color=color,\n",
        "        )\n",
        "        i += 1\n",
        "\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(\"Exchange Rates to USD Per Transaction\")\n",
        "    plt.legend(ncol=2)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def exchange_rate_heat_map(df: pd.DataFrame) -> None:\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    ax = sns.heatmap(\n",
        "        df,\n",
        "        annot=True,\n",
        "        cmap=\"PiYG\",\n",
        "        fmt=\".1f\",\n",
        "        linewidths=0.5,\n",
        "        vmin=-3,\n",
        "        vmax=3,\n",
        "    )\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.xlabel(\"Currencies\")\n",
        "    plt.ylabel(\"Dates\")\n",
        "    plt.title(\"Exchange Rates to USD, Daily % Change\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_usd_conversion(\n",
        "    dataset_dir: str,\n",
        "    plot_exchange_rate_heat_map=False,\n",
        ") -> dict[str, float]:\n",
        "    \"\"\"\n",
        "    Given the name of a dataset, returns a currency conversion dictionary that\n",
        "    will convert every value into U.S. dollars. The keys of the resultant\n",
        "    `currency_conversion` are the magic currency strings used in the AMLworld\n",
        "    datasets\n",
        "    \"\"\"\n",
        "    poor_account_uniqueness = \"Currency Conversion header mismatch\"\n",
        "\n",
        "    # If we can't assume that every currency converts to every other currency\n",
        "    # in the dataset, we need to create a partial conversion map for each\n",
        "    # currency, and then aggregate the results into a single map for U.S.\n",
        "    # dollar conversions\n",
        "    currencies = set()\n",
        "    currency_conversion = {}\n",
        "    conversion_series = {}\n",
        "    with open(dataset_dir, \"r\", encoding=\"utf-8\") as file:\n",
        "        header = True\n",
        "        for line in file:\n",
        "            columns = line.strip().split(\",\")\n",
        "\n",
        "            # Checks that each data set is formatted with the expected data in\n",
        "            # the same position\n",
        "            if header:\n",
        "                header = False\n",
        "                if (\n",
        "                    columns[5] != \"Amount Received\" or\n",
        "                    columns[6] != \"Receiving Currency\" or\n",
        "                    columns[7] != \"Amount Paid\" or\n",
        "                    columns[8] != \"Payment Currency\"\n",
        "                ):\n",
        "                    raise ValueError(poor_account_uniqueness)\n",
        "                continue\n",
        "\n",
        "            dt = datetime.strptime(columns[0], '%Y/%m/%d %H:%M')\n",
        "            date = dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "            if date not in currency_conversion:\n",
        "                currency_conversion[date] = {}\n",
        "\n",
        "            sent_amount = columns[7]\n",
        "            sent_currency = columns[8]\n",
        "            received_amount = columns[5]\n",
        "            received_currency = columns[6]\n",
        "\n",
        "            currencies.add(sent_currency)\n",
        "            currencies.add(received_currency)\n",
        "\n",
        "            # To convert the sent currency to the received currency, multiply\n",
        "            # it by this value\n",
        "            conversion_rate = float(received_amount)/float(sent_amount)\n",
        "\n",
        "            if sent_currency not in currency_conversion[date]:\n",
        "                currency_conversion[date][sent_currency] = {\n",
        "                    sent_currency: 1.0\n",
        "                }\n",
        "            if received_currency not in currency_conversion[date][sent_currency]:\n",
        "                currency_conversion[date][sent_currency][\n",
        "                    received_currency\n",
        "                ] = conversion_rate\n",
        "\n",
        "            # Keep track of all exchange rates\n",
        "            if sent_currency not in conversion_series:\n",
        "                conversion_series[sent_currency] = {}\n",
        "            if received_currency not in conversion_series[sent_currency]:\n",
        "                conversion_series[sent_currency][\n",
        "                    received_currency\n",
        "                ] = []\n",
        "            conversion_series[sent_currency][received_currency].append(\n",
        "                (dt, conversion_rate)\n",
        "            )\n",
        "\n",
        "    if plot_exchange_rate_time_series:\n",
        "        data = []\n",
        "        for currency, values in conversion_series[\"US Dollar\"].items():\n",
        "            for dt, rate in values:\n",
        "                data.append((dt, currency, rate))\n",
        "\n",
        "        dft = pd.DataFrame(data, columns=[\"t\", \"Currency\", \"Exchange Rate\"])\n",
        "        df_pivot = dft.pivot_table(\n",
        "            index=\"t\",\n",
        "            columns=\"Currency\",\n",
        "            values=\"Exchange Rate\",\n",
        "            aggfunc=\"mean\",\n",
        "        )\n",
        "        exchange_rate_time_series(df_pivot, ylabel=\"Exchange Rates\")\n",
        "\n",
        "        dft[\"t\"] = pd.to_datetime(dft[\"t\"])\n",
        "        df_pivot = dft.pivot_table(\n",
        "            index=\"t\",\n",
        "            columns=\"Currency\",\n",
        "            values=\"Exchange Rate\",\n",
        "            aggfunc=\"mean\",\n",
        "        )\n",
        "        df_pivot = df_pivot[df_pivot.index <= \"2022-09-10\"]\n",
        "        df_pivot = df_pivot - df_pivot.mean()\n",
        "        exchange_rate_time_series(df_pivot, ylabel=\"Zero-Mean Exchange Rates\")\n",
        "\n",
        "\n",
        "    if plot_exchange_rate_heat_map:\n",
        "        data = []\n",
        "        for currency, values in conversion_series[\"US Dollar\"].items():\n",
        "            for dt, rate in values:\n",
        "                data.append((dt, currency, rate))\n",
        "\n",
        "        dft = pd.DataFrame(data, columns=[\"t\", \"Currency\", \"Exchange Rate\"])\n",
        "        dft[\"t\"] = pd.to_datetime(dft[\"t\"])\n",
        "        dft.set_index(\"t\", inplace=True)\n",
        "        dft = dft.groupby([pd.Grouper(freq=\"D\"), \"Currency\"]).mean().reset_index()\n",
        "        dft[\"t\"] = dft[\"t\"].dt.date\n",
        "        df_pivot = dft.pivot_table(\n",
        "            index=\"t\",\n",
        "            columns=\"Currency\",\n",
        "            values=\"Exchange Rate\",\n",
        "            aggfunc=\"mean\",\n",
        "        )\n",
        "\n",
        "        # Sets the order to be aesthetic (the latter few days only have US\n",
        "        # Dollar and Euro exchanges), although also eurocentric\n",
        "        all_currencies = list(df_pivot.columns)\n",
        "        fixed_order = [\"US Dollar\", \"Euro\", \"Yuan\"]\n",
        "        remaining_currencies = [c for c in all_currencies if c not in fixed_order]\n",
        "        random.shuffle(remaining_currencies)\n",
        "\n",
        "        sorted_columns = fixed_order + remaining_currencies\n",
        "        df_pivot = df_pivot[sorted_columns]\n",
        "\n",
        "        exchange_rate_heat_map(df_pivot.diff())\n",
        "\n",
        "    # See if we have a complete currency conversion map for U.S. dollars from\n",
        "    # the first day, and use it as an approximation for the entire dataset\n",
        "    usd_conversion = currency_conversion[\n",
        "        list(currency_conversion.keys())[0]\n",
        "    ][\"US Dollar\"]\n",
        "    if set(usd_conversion.keys()) == currencies:\n",
        "        return usd_conversion\n",
        "    return extract_usd_conversion(currency_conversion)\n",
        "\n",
        "\n",
        "def compare_usd_conversions(dataset_a: str, dataset_b: str) -> None:\n",
        "    usd_conversion_a = get_usd_conversion(\n",
        "        os.path.join(data_dir, dataset_a),\n",
        "    )\n",
        "    usd_conversion_b = get_usd_conversion(\n",
        "        os.path.join(data_dir, dataset_b),\n",
        "    )\n",
        "\n",
        "    def compare_dictionaries(\n",
        "        dict_a: dict[str, float], dict_b: dict[str, float],\n",
        "    ) -> dict[str, int]:\n",
        "        if set(dict_a.keys()) != set(dict_b.keys()):\n",
        "            raise ValueError(\n",
        "                \"Cannot compare dictionaries with different sets of keys.\"\n",
        "            )\n",
        "\n",
        "        delta = {}\n",
        "        for k, v in dict_a.items():\n",
        "            delta[k] = f\"{round(100*abs(v - dict_b[k])/v, 2)}%\"\n",
        "\n",
        "        return delta\n",
        "\n",
        "    print(f\"Percent difference between {dataset_a}, {dataset_b}:\")\n",
        "    pprint(compare_dictionaries(usd_conversion_a, usd_conversion_b))\n",
        "\n",
        "\n",
        "if compare_dataset_conversion_rates:\n",
        "    compare_usd_conversions(\"LI-Small_Trans.csv\", \"LI-Medium_Trans.csv\")\n",
        "    compare_usd_conversions(\"LI-Medium_Trans.csv\", \"LI-Large_Trans.csv\")\n",
        "\n",
        "if get_new_usd_conversion_rates:\n",
        "    usd_conversion = get_usd_conversion(\n",
        "        os.path.join(data_dir, dataset),\n",
        "        plot_exchange_rate_heat_map=plot_exchange_rate_heat_map,\n",
        "    )\n",
        "else:\n",
        "    usd_conversion = {\n",
        "        \"US Dollar\": 1.0,\n",
        "        \"Euro\": 0.8533787417099838,\n",
        "        \"Yuan\": 6.697677681891531,\n",
        "        \"Yen\": 105.3976841187823,\n",
        "        \"UK Pound\": 0.7739872068230277,\n",
        "        \"Brazil Real\": 5.646327447497649,\n",
        "        \"Australian Dollar\": 1.4127728666786938,\n",
        "        \"Canadian Dollar\": 1.319260431085624,\n",
        "        \"Ruble\": 77.79226317392629,\n",
        "        \"Mexican Peso\": 21.1287988422576,\n",
        "        \"Rupee\": 73.44399970830806,\n",
        "        \"Swiss Franc\": 0.9149993127687566,\n",
        "        \"Shekel\": 3.3769999188170305,\n",
        "        \"Saudi Riyal\": 3.751098012020342,\n",
        "        \"Bitcoin\": 8.333333333333333e-05,\n",
        "    }"
      ],
      "metadata": {
        "id": "VfD7pEbgCd46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Customer Snapshots\n",
        "\n",
        "Creates a new dataframe where each row is a customer, and each column is an aggregate of that customer's transaction data. The goal is to train a non-graph-based classification model on customer snapshots that can detect good (licit) customers, and to filter these out prior to feeding data into the graph-based model.\n",
        "\n",
        "This is meant to be a form of knowledge-driven undersampling, where the filter model will detect licit customers (positives), maximizing the true negative rate (illicit customers) to ensure these make it to the graph model."
      ],
      "metadata": {
        "id": "Ck7weJR3zlbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_customer_data(\n",
        "    dataset: str,\n",
        "    conversion: dict[str, float],\n",
        ") -> pd.DataFrame:\n",
        "    hash_map = {}\n",
        "    base_customer_data = {\n",
        "        \"n_sent\": 0,\n",
        "        \"total_sent\": 0,\n",
        "        \"first_sent\": float(\"inf\"),\n",
        "        \"last_sent\": float(\"-inf\"),\n",
        "        \"sent_type_counts\": dict(),\n",
        "        \"n_destinations\": set(),\n",
        "        \"n_received\": 0,\n",
        "        \"total_received\": 0,\n",
        "        \"first_received\": float(\"inf\"),\n",
        "        \"last_received\": float(\"-inf\"),\n",
        "        \"received_type_counts\": dict(),\n",
        "        \"n_sources\": set(),\n",
        "        \"is_launderer\": 0,\n",
        "    }\n",
        "    payment_type_map = {\n",
        "        \"Reinvestment\": \"reinvestment\",\n",
        "        \"Cheque\": \"cheque\",\n",
        "        \"ACH\": \"ach\",\n",
        "        \"Credit Card\": \"credit_card\",\n",
        "        \"Wire\": \"wire\",\n",
        "        \"Cash\": \"cash\",\n",
        "        \"Bitcoin\": \"bitcoin\",\n",
        "    }\n",
        "    currency_map = {\n",
        "        \"US Dollar\": \"usd\",\n",
        "        \"Euro\": \"euro\",\n",
        "        \"Yuan\": \"yuan\",\n",
        "        \"Yen\": \"yen\",\n",
        "        \"UK Pound\": \"pound\",\n",
        "        \"Brazil Real\": \"real\",\n",
        "        \"Australian Dollar\": \"ausd\",\n",
        "        \"Canadian Dollar\": \"cd\",\n",
        "        \"Ruble\": \"ruble\",\n",
        "        \"Mexican Peso\": \"peso\",\n",
        "        \"Rupee\": \"rupee\",\n",
        "        \"Swiss Franc\": \"franc\",\n",
        "        \"Shekel\": \"shekel\",\n",
        "        \"Saudi Riyal\": \"riyal\",\n",
        "        \"Bitcoin\": \"btc\",\n",
        "    }\n",
        "    day_names = [x.lower() for x in list(calendar.day_name)]\n",
        "    hours = [f\"hour_{x}\" for x in range(24)]\n",
        "\n",
        "    # Add sent and received currency and day of week columns; currency will be\n",
        "    # one-hot encoded for which currencies each customer uses, day of week will\n",
        "    # be the count of sent and received transactions on those days\n",
        "    for prefix in [\"sent\", \"received\"]:\n",
        "        for k, v in currency_map.items():\n",
        "            base_customer_data[f\"{prefix}_{v}\"] = 0\n",
        "        for day_of_week in day_names:\n",
        "            base_customer_data[f\"{prefix}_{day_of_week}\"] = 0\n",
        "        for hour in hours:\n",
        "            base_customer_data[f\"{prefix}_{hour}\"] = 0\n",
        "\n",
        "    poor_account_uniqueness = \"Customer Aggregation header mismatch\"\n",
        "\n",
        "    with open(\n",
        "        os.path.join(data_dir, dataset), \"r\", encoding=\"utf-8\",\n",
        "    ) as file:\n",
        "        header = True\n",
        "        for line in file:\n",
        "            columns = line.strip().split(\",\")\n",
        "\n",
        "            # Checks that each data set is formatted with the expected data in\n",
        "            # the same position\n",
        "            if header:\n",
        "                header = False\n",
        "                if (\n",
        "                    columns[1] != \"From Bank\" or\n",
        "                    columns[2] != \"Account\" or\n",
        "                    columns[3] != \"To Bank\" or\n",
        "                    columns[4] != \"Account\" or\n",
        "                    columns[5] != \"Amount Received\" or\n",
        "                    columns[6] != \"Receiving Currency\" or\n",
        "                    columns[7] != \"Amount Paid\" or\n",
        "                    columns[8] != \"Payment Currency\" or\n",
        "                    columns[9] != \"Payment Format\" or\n",
        "                    columns[10] != \"Is Laundering\"\n",
        "                ):\n",
        "                    raise ValueError(poor_account_uniqueness)\n",
        "                continue\n",
        "\n",
        "            id_from = f\"{columns[1]}_{columns[2]}\"\n",
        "            id_to = f\"{columns[3]}_{columns[4]}\"\n",
        "\n",
        "            if id_from not in hash_map:\n",
        "                hash_map[id_from] = deepcopy(base_customer_data)\n",
        "            if id_to not in hash_map:\n",
        "                hash_map[id_to] = deepcopy(base_customer_data)\n",
        "\n",
        "            hash_map[id_from][\"n_destinations\"].add(id_to)\n",
        "            hash_map[id_to][\"n_sources\"].add(id_from)\n",
        "\n",
        "            sent_currency = currency_map[columns[8]]\n",
        "            hash_map[id_from][f\"sent_{sent_currency}\"] = 1\n",
        "            received_currency = currency_map[columns[6]]\n",
        "            hash_map[id_from][f\"received_{received_currency}\"] = 1\n",
        "\n",
        "            amount_sent = round(float(columns[7])/conversion[columns[8]], 1)\n",
        "            amount_received = round(float(columns[5])/conversion[columns[6]], 1)\n",
        "\n",
        "            hash_map[id_from][\"n_sent\"] += 1\n",
        "            hash_map[id_to][\"n_received\"] += 1\n",
        "            hash_map[id_from][\"total_sent\"] += amount_sent\n",
        "            hash_map[id_to][\"total_received\"] += amount_received\n",
        "\n",
        "\n",
        "            dt = datetime.strptime(columns[0], '%Y/%m/%d %H:%M')\n",
        "            unix_time = int(dt.timestamp())\n",
        "            day_of_week = dt.strftime(\"%A\").lower()\n",
        "            hour_of_day = f\"hour_{dt.hour}\"\n",
        "\n",
        "            if unix_time < hash_map[id_from][\"first_sent\"]:\n",
        "                hash_map[id_from][\"first_sent\"] = unix_time\n",
        "            if unix_time > hash_map[id_from][\"last_sent\"]:\n",
        "                hash_map[id_from][\"last_sent\"] = unix_time\n",
        "            if unix_time < hash_map[id_to][\"first_received\"]:\n",
        "                hash_map[id_to][\"first_received\"] = unix_time\n",
        "            if unix_time > hash_map[id_to][\"last_received\"]:\n",
        "                hash_map[id_to][\"last_received\"] = unix_time\n",
        "\n",
        "            hash_map[id_from][f\"sent_{day_of_week}\"] += 1\n",
        "            hash_map[id_to][f\"received_{day_of_week}\"] += 1\n",
        "\n",
        "            hash_map[id_from][f\"sent_{hour_of_day}\"] += 1\n",
        "            hash_map[id_to][f\"received_{hour_of_day}\"] += 1\n",
        "\n",
        "            payment_type = payment_type_map[columns[9]]\n",
        "            if payment_type not in hash_map[id_from][\"sent_type_counts\"]:\n",
        "                hash_map[id_from][\"sent_type_counts\"][payment_type] = 1\n",
        "            else:\n",
        "                hash_map[id_from][\"sent_type_counts\"][payment_type] += 1\n",
        "            if payment_type not in hash_map[id_to][\"received_type_counts\"]:\n",
        "                hash_map[id_to][\"received_type_counts\"][payment_type] = 1\n",
        "            else:\n",
        "                hash_map[id_to][\"received_type_counts\"][payment_type] += 1\n",
        "\n",
        "            # TODO: do we consider both the sender and receiver a money\n",
        "            # launderer for participating in the transaction?\n",
        "            if int(columns[10]):\n",
        "                hash_map[id_from][\"is_launderer\"] = 1\n",
        "                hash_map[id_to][\"is_launderer\"] = 1\n",
        "\n",
        "    # Might want to transform this to a dataframe\n",
        "    return pd.DataFrame.from_dict(hash_map, orient=\"index\")\n",
        "\n",
        "\n",
        "dfc = aggregate_customer_data(\n",
        "    dataset=dataset,\n",
        "    conversion=usd_conversion,\n",
        ")\n",
        "\n",
        "# Name the index; it shouldn't be used in an analysis\n",
        "dfc.index.name = \"bank_account\"\n",
        "\n",
        "# Don't keep track of sets of sources or destinations, just the number of each\n",
        "dfc[\"n_destinations\"] = dfc[\"n_destinations\"].apply(len)\n",
        "dfc[\"n_sources\"] = dfc[\"n_sources\"].apply(len)\n",
        "\n",
        "# Convert the set of currencies into one-hot encoded columns\n",
        "\n",
        "# Get the amount of time the account has been active.\n",
        "# Then remove the first and last sent and received columns\n",
        "last = [\"last_sent\", \"last_received\"]\n",
        "first = [\"first_sent\", \"first_received\"]\n",
        "dfc[\"transaction_length\"] = dfc[last].max(axis=1) - dfc[first].min(axis=1)\n",
        "dfc = dfc.drop(columns=last+first)\n",
        "\n",
        "# Include the average sent and received transaction sizes\n",
        "dfc.fillna(0, inplace=True)\n",
        "dfc[\"avg_sent\"] = np.where(\n",
        "    dfc[\"n_sent\"] < 1,\n",
        "    0,\n",
        "    dfc[\"total_sent\"] / dfc[\"n_sent\"],\n",
        ")\n",
        "dfc[\"avg_received\"] = np.where(\n",
        "    dfc[\"n_received\"] < 1,\n",
        "    0,\n",
        "    dfc[\"total_received\"] / dfc[\"n_received\"],\n",
        ")\n",
        "\n",
        "# Since we are dealing with currency, round each column to 2 decimal places\n",
        "dfc = dfc.round(2)\n",
        "\n",
        "# Create a separate column or feature for each payment type count\n",
        "dfc_sent_types = (\n",
        "    pd.json_normalize(dfc[\"sent_type_counts\"])\n",
        "        .add_prefix(\"n_sent_\")\n",
        "        .set_index(dfc.index)\n",
        ")\n",
        "dfc_sent_types.fillna(0, inplace=True)\n",
        "dfc = dfc.drop(columns=[\"sent_type_counts\"]).join(dfc_sent_types)\n",
        "dfc_received_types = (\n",
        "    pd.json_normalize(dfc[\"received_type_counts\"])\n",
        "    .add_prefix(\"n_received_\")\n",
        "    .set_index(dfc.index)\n",
        ")\n",
        "dfc_received_types.fillna(0, inplace=True)\n",
        "dfc = dfc.drop(columns=[\"received_type_counts\"]).join(dfc_received_types)\n",
        "\n",
        "# The \"positive\" for a knowledge-drive undersampler is one who does not launder\n",
        "dfc[\"is_good_customer\"] = dfc[\"is_launderer\"] ^ 1"
      ],
      "metadata": {
        "id": "Bb_V7fURFQ9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prints a single row in the dataframe, in case it helps to spot check the data\n",
        "row_dict = {\n",
        "    col: list(val.values())[0]\n",
        "    if isinstance(val, dict)\n",
        "    else val\n",
        "    for col, val in dfc.iloc[0].to_dict().items()\n",
        "}\n",
        "pprint(row_dict)"
      ],
      "metadata": {
        "id": "vgacB_H_qSsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_selector = widgets.Dropdown(\n",
        "    options=dfc.columns,\n",
        "    description=\"Feature:\",\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "n_drop_slider = widgets.IntSlider(\n",
        "    value=25,\n",
        "    min=0,\n",
        "    max=10000,\n",
        "    step=1,\n",
        "    description=\"Exclude top:\",\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "n_bins_slider = widgets.IntSlider(\n",
        "    value=50,\n",
        "    min=2,\n",
        "    max=250,\n",
        "    step=1,\n",
        "    description=\"Histogram bins:\",\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "\n",
        "def plot_customer_histogram(column, n_drop=25, n_bins=50):\n",
        "    dfiltered = dfc.drop(dfc.nlargest(n_drop, column).index)[column]\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.hist(dfiltered, bins=n_bins, alpha=0.7, edgecolor='black')\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(f\"Histogram of {column}\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "widgets.interactive(\n",
        "    plot_customer_histogram,\n",
        "    column=column_selector,\n",
        "    n_drop=n_drop_slider,\n",
        "    n_bins=n_bins_slider,\n",
        ")"
      ],
      "metadata": {
        "id": "g1HExPOCpz8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: add a noramlization step here that normalizes each column, do we need\n",
        "# to use a log-based normalization, given how imbalanced the distributions in\n",
        "# the above histograms are?"
      ],
      "metadata": {
        "id": "4pXe2kA5Ivo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: can we abstract this portion of the code away, similar to what is done\n",
        "# this Kaggle notebook:\n",
        "#\n",
        "#   - <https://www.kaggle.com/code/caesarmario/listen-to-your-heart-a-disease-prediction>\n",
        "#\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import (\n",
        "    average_precision_score,\n",
        "    auc,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    precision_recall_curve,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "set_model = \"xgb\"\n",
        "\n",
        "\n",
        "X = dfc.drop(columns=[\"is_launderer\", \"is_good_customer\"])\n",
        "y = dfc[\"is_good_customer\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y,\n",
        ")\n",
        "\n",
        "if set_model == \"logistic\":\n",
        "    model = LogisticRegression(\n",
        "        solver=\"liblinear\",\n",
        "        class_weight=\"balanced\",\n",
        "        max_iter=500,\n",
        "    )\n",
        "elif set_model == \"xgb\":\n",
        "    model = XGBClassifier(\n",
        "        objective=\"binary:logistic\",\n",
        "        # scale_pos_weight=y_train.value_counts()[0] / y_train.value_counts()[1],\n",
        "        eval_metric=\"logloss\",\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=8,\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_test = model.predict(X_test)"
      ],
      "metadata": {
        "id": "4vRrm45i3vkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import interact, FloatSlider\n",
        "\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "def update_confusion_matrix(threshold):\n",
        "    y_pred_test = (y_probs >= threshold).astype(int)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_test).ravel()\n",
        "\n",
        "    print(f\"Threshold: {threshold:.3f}\")\n",
        "    print(f\"True Negatives (TN): {tn}\")\n",
        "    print(f\"False Positives (FP): {fp}\")\n",
        "    print(f\"False Negatives (FN): {fn}\")\n",
        "    print(f\"True Positives (TP): {tp}\")\n",
        "\n",
        "\n",
        "interact(\n",
        "    update_confusion_matrix,\n",
        "    threshold=FloatSlider(\n",
        "        min=0.0,\n",
        "        max=1.0,\n",
        "        step=0.001,\n",
        "        value=0.8,\n",
        "        readout_format=\".3f\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "7Dq3XIVrYnd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "id": "5BffXewj7iuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fpr, tpr, _ = roc_curve(y_test, y_pred_test)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")  # Random classifier\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IS-Y7fmQCend"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_test)\n",
        "avg_precision = average_precision_score(y_test, y_pred_test)\n",
        "\n",
        "# Plot the PR curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, marker='.', label=f'AP = {avg_precision:.2f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aH2P7KV7TOJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FuXJgRr5ZMd"
      },
      "outputs": [],
      "source": [
        "# Some next steps to consider:\n",
        "#\n",
        "#   - Confirm that `from_unique` is the same for the same from bank and account\n",
        "#   - For each `from_unique` get the first and last transaction\n",
        "#   - Show a distribution of this to get a sense of customer history\n",
        "#   - Do logistic regression on these customer snapshots\n",
        "#\n",
        "#   - Be sure to one-hot encode categorical features\n",
        "#\n",
        "# Once categorical data is converted into numerical data, do:\n",
        "#\n",
        "#   df.corr()\n",
        "#"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}