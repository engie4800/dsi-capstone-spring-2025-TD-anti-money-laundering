{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97e79eae",
   "metadata": {},
   "source": [
    "# Example notebook for GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fa2556",
   "metadata": {},
   "source": [
    "## Notebook configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8609be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some libraries\n",
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set up system path, and import our custom modules\n",
    "# helpers: for cell timer\n",
    "# pipeline: all data preprocessing\n",
    "# model: for GNN model & trainer\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\", \"..\", \"src\")))\n",
    "from helpers import add_cell_timer\n",
    "from pipeline import BaseModelPipeline\n",
    "from pipeline.gnn_pipeline import GNNModelPipeline\n",
    "from pipeline.catboost_pipeline import CatBoostPipeline\n",
    "import model\n",
    "add_cell_timer()\n",
    "\n",
    "data_file = \"../../data/subset_transactions2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fff0373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### For reloading updated modules\n",
    "# import pipeline.gnn_pipeline\n",
    "# import importlib\n",
    "# importlib.reload(pipeline.gnn_pipeline)\n",
    "# from pipeline.gnn_pipeline import GNNModelPipeline\n",
    "\n",
    "# ### For reloading updated modules\n",
    "# importlib.reload(pipeline)\n",
    "# from pipeline import ModelPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b514b34b",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "716b394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = GNNModelPipeline(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35981a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sophi\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\sophi\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but OneHotEncoder was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pl.rename_columns()\n",
    "pl.drop_duplicates()\n",
    "pl.check_for_null()\n",
    "pl.extract_currency_features()\n",
    "pl.extract_time_features()\n",
    "pl.create_unique_ids()\n",
    "pl.extract_additional_time_features()\n",
    "pl.cyclical_encoding()\n",
    "pl.apply_one_hot_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2528a9",
   "metadata": {},
   "source": [
    "## Split data into train/val/test, and continue with split-specific feature engineering\n",
    "There are some features that, if engineered or standardized using the whole dataset, could result in data leakage between our train/val/test sets. Therefore, we must split the data prior to these calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed69d9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def split_train_test_val(self, X_cols=None, y_col=\"is_laundering\", test_size=0.15, val_size=0.15, split_type=\"temporal_agg\"):\n",
      "            return super().split_train_test_val(X_cols, y_col, test_size, val_size, split_type)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "# Check default split mehod for GNN\n",
    "print(inspect.getsource(pl.split_train_test_val))\n",
    "\n",
    "# Temporal split for edges\n",
    "pl.split_train_test_val()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b92f01",
   "metadata": {},
   "source": [
    "### Create node features\n",
    "Node features are specific to accounts, and include graph based features like pagerank and degree centrality, as well as some aggregate statistics such as net flow (total amount sent-total amount received for a specific account). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "381f8ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Computed node features for train with 107090 nodes.\n",
      "✅ Computed node features for val with 107355 nodes.\n",
      "✅ Computed node features for test with 107583 nodes.\n",
      "Index(['node_id', 'degree_centrality', 'pagerank', 'net_flow', 'avg_txn_out',\n",
      "       'avg_txn_in', 'std_txn_out', 'std_txn_in', 'num_unique_out_partners',\n",
      "       'num_unique_in_partners'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Compute node features split-specifically\n",
    "pl.compute_split_specific_node_features()\n",
    "\n",
    "# Scale only relevant node features (others like pagerank left raw)\n",
    "pl.scale_node_data_frames()\n",
    "\n",
    "print(pl.train_nodes.columns) # print node features to peek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdfbfe2",
   "metadata": {},
   "source": [
    "### Create graph objects (GNN specific processes)\n",
    "The `split_train_test_val_graph()` function uses the data split above, and creates PyG-style Data objects. PyG-style Data objects have features like:\n",
    "\n",
    "- x: node (account) features (without column for \"node_id\", mind you--so these must be properly sorted and align with our unique edge indexers)\n",
    "- edge_index: a [2, num_transactions] tensor containing the accounts involved in each transaction\n",
    "- edge_attr: the edge (transaction) features, listed above, including things like amount, temporal features, and payment type\n",
    "- y: our labels -- 'is_laundering' column, associated with each transaction\n",
    "\n",
    "Another feature of our `split_train_test_val_graph` function is reordering columns such that we have 'edge_id' as the first column -- this is important for how our model works, since we use edge_id to determine which transactions to evaluate during model training, but then drop the column before passing the transactions into the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68bcb63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['day_cos', 'day_sin', 'edge_id', 'hour_of_day', 'is_weekend', 'log_exchange_rate', 'payment_type_ACH', 'payment_type_Bitcoin', 'payment_type_Cash', 'payment_type_Cheque', 'payment_type_Credit Card', 'payment_type_Reinvestment', 'payment_type_Wire', 'received_amount', 'received_currency_Australian Dollar', 'received_currency_Bitcoin', 'received_currency_Brazil Real', 'received_currency_Canadian Dollar', 'received_currency_Euro', 'received_currency_Mexican Peso', 'received_currency_Ruble', 'received_currency_Rupee', 'received_currency_Saudi Riyal', 'received_currency_Shekel', 'received_currency_Swiss Franc', 'received_currency_UK Pound', 'received_currency_US Dollar', 'received_currency_Yen', 'received_currency_Yuan', 'sent_amount', 'sent_amount_usd', 'sent_currency_Australian Dollar', 'sent_currency_Bitcoin', 'sent_currency_Brazil Real', 'sent_currency_Canadian Dollar', 'sent_currency_Euro', 'sent_currency_Mexican Peso', 'sent_currency_Ruble', 'sent_currency_Rupee', 'sent_currency_Saudi Riyal', 'sent_currency_Shekel', 'sent_currency_Swiss Franc', 'sent_currency_UK Pound', 'sent_currency_US Dollar', 'sent_currency_Yen', 'sent_currency_Yuan', 'time_diff_from', 'time_diff_to', 'time_of_day_cos', 'time_of_day_sin', 'timestamp_int', 'timestamp_scaled', 'turnaround_time']\n",
      "['edge_id', 'sent_currency_Euro', 'sent_currency_Mexican Peso', 'log_exchange_rate', 'payment_type_Reinvestment', 'received_currency_Saudi Riyal', 'received_currency_Yen', 'time_of_day_cos', 'sent_currency_Australian Dollar', 'received_currency_Yuan', 'sent_currency_Canadian Dollar', 'sent_currency_UK Pound', 'payment_type_Cheque', 'sent_currency_Rupee', 'received_currency_Shekel', 'day_sin', 'sent_currency_Brazil Real', 'sent_currency_Bitcoin', 'received_currency_Mexican Peso', 'sent_currency_Yuan', 'timestamp_scaled', 'received_currency_UK Pound', 'time_of_day_sin', 'received_currency_Euro', 'sent_currency_Saudi Riyal', 'received_currency_US Dollar', 'payment_type_Cash', 'payment_type_Credit Card', 'received_currency_Bitcoin', 'time_diff_to', 'payment_type_ACH', 'payment_type_Wire', 'time_diff_from', 'received_currency_Ruble', 'sent_currency_Ruble', 'sent_currency_Swiss Franc', 'received_currency_Swiss Franc', 'sent_currency_Shekel', 'sent_currency_US Dollar', 'turnaround_time', 'received_currency_Brazil Real', 'received_currency_Rupee', 'received_currency_Australian Dollar', 'sent_amount_usd', 'payment_type_Bitcoin', 'sent_currency_Yen', 'day_cos', 'received_currency_Canadian Dollar']\n"
     ]
    }
   ],
   "source": [
    "print(pl.X_cols)\n",
    "edge_feats = ['edge_id'] + list(set(pl.X_cols) - set(['timestamp_int','hour_of_day','is_weekend','edge_id','sent_amount','received_amount']))\n",
    "print(edge_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dd271cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['edge_id', 'sent_currency_Euro', 'sent_currency_Mexican Peso', 'log_exchange_rate', 'payment_type_Reinvestment', 'received_currency_Saudi Riyal', 'received_currency_Yen', 'time_of_day_cos', 'sent_currency_Australian Dollar', 'received_currency_Yuan', 'sent_currency_Canadian Dollar', 'sent_currency_UK Pound', 'payment_type_Cheque', 'sent_currency_Rupee', 'received_currency_Shekel', 'day_sin', 'sent_currency_Brazil Real', 'sent_currency_Bitcoin', 'received_currency_Mexican Peso', 'sent_currency_Yuan', 'timestamp_scaled', 'received_currency_UK Pound', 'time_of_day_sin', 'received_currency_Euro', 'sent_currency_Saudi Riyal', 'received_currency_US Dollar', 'payment_type_Cash', 'payment_type_Credit Card', 'received_currency_Bitcoin', 'time_diff_to', 'payment_type_ACH', 'payment_type_Wire', 'time_diff_from', 'received_currency_Ruble', 'sent_currency_Ruble', 'sent_currency_Swiss Franc', 'received_currency_Swiss Franc', 'sent_currency_Shekel', 'sent_currency_US Dollar', 'turnaround_time', 'received_currency_Brazil Real', 'received_currency_Rupee', 'received_currency_Australian Dollar', 'sent_amount_usd', 'payment_type_Bitcoin', 'sent_currency_Yen', 'day_cos', 'received_currency_Canadian Dollar']\n"
     ]
    }
   ],
   "source": [
    "# Convert into PyG-style Data objects\n",
    "pl.split_train_test_val_graph(edge_features=edge_feats)\n",
    "print(pl.edge_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74d843a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sent_amount_usd', 'timestamp_scaled', 'time_diff_from', 'time_diff_to', 'turnaround_time']\n"
     ]
    }
   ],
   "source": [
    "pl.scale_edge_features(edge_features_to_scale=['sent_amount_usd','timestamp_scaled','time_diff_from','time_diff_to','turnaround_time'])\n",
    "print(pl.scaled_edge_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7eeeb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data loaders for training\n",
    "pl.get_data_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54077770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def initialize_training(\n",
      "        self,\n",
      "        threshold: float=0.5,\n",
      "        epochs: int=50,\n",
      "        patience: int=10,\n",
      "    ) -> None:\n",
      "        \"\"\"Setup the model pipeline for training: metrics, model,\n",
      "        optimizer, scheduler, and criterion\n",
      "        \"\"\"\n",
      "        self.threshold = threshold\n",
      "        self.epochs = epochs\n",
      "        self.patience = patience\n",
      "        \n",
      "        \n",
      "\n",
      "        # Since `initialize_training` is run after preprocessing is\n",
      "        # done, we can define the node and edge features here. This\n",
      "        # does assume that column ordering between data frames and\n",
      "        # tensors is preserved, and it removes node and edge id\n",
      "        # TODO: ran into issue with this line bc node_id has already been dropped\n",
      "        if 'node_id' in self.nodes.columns:\n",
      "            self.node_feature_labels = self.nodes.drop(columns=\"node_id\").columns\n",
      "        else:\n",
      "            self.node_feature_labels = self.nodes.columns\n",
      "        self.edge_feature_labels = self.df[self.edge_features].drop(columns=\"edge_id\").columns\n",
      "\n",
      "\n",
      "\n",
      "        # Model setup\n",
      "        num_edge_features = self.train_data.edge_attr.shape[1]-1  # num edge feats - edge_id\n",
      "        num_node_features = self.train_data.x.shape[1]\n",
      "        self.model = GINe(n_node_feats=num_node_features, n_edge_feats=num_edge_features).to(self.device)\n",
      "        self.trainer = GNNTrainer(self.model, self)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(pl.initialize_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a4f6451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sophi\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pl.initialize_training(epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b7f5fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GINe(\n",
      "  (node_emb): Linear(in_features=9, out_features=100, bias=True)\n",
      "  (edge_emb): Linear(in_features=47, out_features=100, bias=True)\n",
      "  (convs): ModuleList(\n",
      "    (0-1): 2 x GINEConv(nn=Sequential(\n",
      "      (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    ))\n",
      "  )\n",
      "  (emlps): ModuleList(\n",
      "    (0-1): 2 x Sequential(\n",
      "      (0): Linear(in_features=300, out_features=100, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0-1): 2 x BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.10527690625126304, inplace=False)\n",
      "    (3): Linear(in_features=50, out_features=25, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.10527690625126304, inplace=False)\n",
      "    (6): Linear(in_features=25, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pl.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ee7889b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:   0%|          | 0/107 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "'NeighborSampler' requires either 'pyg-lib' or 'torch-sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\repos\\dsi-capstone-spring-2025-TD-anti-money-laundering\\Code\\src\\model\\__init__.py:231\u001b[0m, in \u001b[0;36mGNNTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mf1_score\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mpr_auc\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m--> 231\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Training\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# Identify  batch seed transaction ids for loss calculation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sophi\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sophi\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\sophi\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\sophi\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sophi\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\torch_geometric\\loader\\link_loader.py:211\u001b[0m, in \u001b[0;36mLinkLoader.collate_fn\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Samples a subgraph from a batch of input edges.\"\"\"\u001b[39;00m\n\u001b[0;32m    209\u001b[0m input_data: EdgeSamplerInput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_data[index]\n\u001b[1;32m--> 211\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlink_sampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_from_edges\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_sampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneg_sampling\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_per_worker:  \u001b[38;5;66;03m# Execute `filter_fn` in the worker process\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_fn(out)\n",
      "File \u001b[1;32mc:\\Users\\sophi\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\torch_geometric\\sampler\\neighbor_sampler.py:334\u001b[0m, in \u001b[0;36mNeighborSampler.sample_from_edges\u001b[1;34m(self, inputs, neg_sampling)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msample_from_edges\u001b[39m(\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    331\u001b[0m     inputs: EdgeSamplerInput,\n\u001b[0;32m    332\u001b[0m     neg_sampling: Optional[NegativeSampling] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    333\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[SamplerOutput, HeteroSamplerOutput]:\n\u001b[1;32m--> 334\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43medge_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisjoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_sampling\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubgraph_type \u001b[38;5;241m==\u001b[39m SubgraphType\u001b[38;5;241m.\u001b[39mbidirectional:\n\u001b[0;32m    337\u001b[0m         out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto_bidirectional()\n",
      "File \u001b[1;32mc:\\Users\\sophi\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\torch_geometric\\sampler\\neighbor_sampler.py:740\u001b[0m, in \u001b[0;36medge_sample\u001b[1;34m(inputs, sample_fn, num_nodes, disjoint, node_time, neg_sampling)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_label_time \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Always disjoint.\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     seed_time \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([src_time, dst_time])\n\u001b[1;32m--> 740\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43msample_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;66;03m# Enhance `out` by label information ##################################\u001b[39;00m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m neg_sampling \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m neg_sampling\u001b[38;5;241m.\u001b[39mis_binary():\n",
      "File \u001b[1;32mc:\\Users\\sophi\\anaconda3\\envs\\capstone_env\\lib\\site-packages\\torch_geometric\\sampler\\neighbor_sampler.py:508\u001b[0m, in \u001b[0;36mNeighborSampler._sample\u001b[1;34m(self, seed, seed_time, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m     num_sampled_nodes \u001b[38;5;241m=\u001b[39m num_sampled_edges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 508\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requires \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    509\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meither \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyg-lib\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch-sparse\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m SamplerOutput(\n\u001b[0;32m    512\u001b[0m     node\u001b[38;5;241m=\u001b[39mnode,\n\u001b[0;32m    513\u001b[0m     row\u001b[38;5;241m=\u001b[39mrow,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    518\u001b[0m     num_sampled_edges\u001b[38;5;241m=\u001b[39mnum_sampled_edges,\n\u001b[0;32m    519\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: 'NeighborSampler' requires either 'pyg-lib' or 'torch-sparse'"
     ]
    }
   ],
   "source": [
    "pl.trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
