{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "97e79eae",
      "metadata": {
        "id": "97e79eae"
      },
      "source": [
        "# Example notebook for GNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1fa2556",
      "metadata": {
        "id": "d1fa2556"
      },
      "source": [
        "## Notebook configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For VSCode"
      ],
      "metadata": {
        "id": "9kH4WAR3jxvH"
      },
      "id": "9kH4WAR3jxvH"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a8609be0",
      "metadata": {
        "id": "a8609be0"
      },
      "outputs": [],
      "source": [
        "# Load some libraries\n",
        "# import os\n",
        "# import sys\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# # Set up system path, and import our custom modules\n",
        "# # helpers: for cell timer\n",
        "# # pipeline: all data preprocessing\n",
        "# # model: for GNN model & trainer\n",
        "# sys.path.append(os.path.abspath(os.path.join(\"..\", \"..\", \"src\")))\n",
        "# from helpers import add_cell_timer\n",
        "# from pipeline import ModelPipeline\n",
        "# from pipeline.gnn_pipeline import GNNPipeline\n",
        "# from pipeline.catboost_pipeline import CatBoostPipeline\n",
        "# import model\n",
        "# add_cell_timer()\n",
        "\n",
        "# data_file = \"../../data/subset_transactions2.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For colab"
      ],
      "metadata": {
        "id": "E6wnGmGOj06b"
      },
      "id": "E6wnGmGOj06b"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from tabulate import tabulate\n",
        "from tqdm import trange\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "from time import monotonic\n",
        "from pprint import pprint\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.nn import BCEWithLogitsLoss, Sequential, Linear, ReLU\n",
        "!pip install torch==2.5.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install torch-scatter torch-sparse pyg-lib torch-geometric \\\n",
        "  -f https://data.pyg.org/whl/torch-2.5.1+cu118.html\n",
        "\n",
        "# !pip install torch==2.5.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "# !pip install torch-scatter torch-sparse pyg-lib torch-geometric -f https://data.pyg.org/whl/torch-2.5.1+cpu.html\n",
        "\n",
        "from torch_geometric.nn import GINEConv\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader, LinkNeighborLoader\n",
        "!pip install torchmetrics\n",
        "from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score, BinaryAveragePrecision\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "content_base = \"/content/drive\"\n",
        "drive.mount(content_base)\n",
        "\n",
        "# Project data\n",
        "data_dir = os.path.join(content_base, \"My Drive/Capstone/data\")\n",
        "data_file = os.path.join(data_dir, \"subset_transactions2.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yLIxI2UjvmC",
        "outputId": "67ef22e6-c421-419c-f20e-4abe4ee7a1c0"
      },
      "id": "6yLIxI2UjvmC",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (2.5.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (11.8.86)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1) (3.0.2)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Using cached https://data.pyg.org/whl/torch-2.5.0%2Bcu118/torch_scatter-2.1.2%2Bpt25cu118-cp311-cp311-linux_x86_64.whl (10.3 MB)\n",
            "Collecting torch-sparse\n",
            "  Using cached https://data.pyg.org/whl/torch-2.5.0%2Bcu118/torch_sparse-0.6.18%2Bpt25cu118-cp311-cp311-linux_x86_64.whl (5.0 MB)\n",
            "Collecting pyg-lib\n",
            "  Using cached https://data.pyg.org/whl/torch-2.5.0%2Bcu118/pyg_lib-0.4.0%2Bpt25cu118-cp311-cp311-linux_x86_64.whl (2.6 MB)\n",
            "Collecting torch-geometric\n",
            "  Using cached torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.14.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Using cached torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "Installing collected packages: torch-scatter, pyg-lib, torch-sparse, torch-geometric\n",
            "Successfully installed pyg-lib-0.4.0+pt25cu118 torch-geometric-2.6.1 torch-scatter-2.1.2+pt25cu118 torch-sparse-0.6.18+pt25cu118\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.5.1+cu118)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.8.86)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.14.3 torchmetrics-1.7.1\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colaboratory executes in an environment with a file system\n",
        "# that has a Linux topography, but where the user should work under\n",
        "# the `/content` directory\n",
        "COLAB_ROOT = \"/content\"\n",
        "\n",
        "REPO_URL = \"https://github.com/engie4800/dsi-capstone-spring-2025-TD-anti-money-laundering.git\"\n",
        "REPO_ROOT = os.path.join(COLAB_ROOT, REPO_URL.split(\"/\")[-1].split(\".\")[0])\n",
        "REPO_BRANCH = \"sophie\"\n",
        "\n",
        "# Clones the repository at `/content/dsi-capstone-spring-2025-TD-anti-money-laundering`\n",
        "if not os.path.exists(REPO_ROOT):\n",
        "  os.chdir(COLAB_ROOT)\n",
        "  !git clone {REPO_URL}\n",
        "\n",
        "# Pulls the latest code from the provided branch and adds the\n",
        "# analysis pipeline source code to the Python system path\n",
        "os.chdir(REPO_ROOT)\n",
        "!git pull\n",
        "!git checkout {REPO_BRANCH}\n",
        "sys.path.append(os.path.join(REPO_ROOT, \"Code/src\"))\n",
        "os.chdir(COLAB_ROOT)\n",
        "\n",
        "# Set up system path, and import our custom modules\n",
        "# helpers: for cell timer\n",
        "# pipeline: all data preprocessing\n",
        "# model: for GNN model & trainer\n",
        "sys.path.append(os.path.abspath(os.path.join(\"..\", \"..\", \"src\")))\n",
        "from helpers import add_cell_timer\n",
        "from pipeline import ModelPipeline\n",
        "from pipeline.gnn_pipeline import GNNPipeline\n",
        "from pipeline.catboost_pipeline import CatBoostPipeline\n",
        "import model\n",
        "add_cell_timer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXqs1OtakBjl",
        "outputId": "24842cff-c7fb-4cb0-ad48-ab09c52718a7"
      },
      "id": "gXqs1OtakBjl",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dsi-capstone-spring-2025-TD-anti-money-laundering'...\n",
            "remote: Enumerating objects: 1124, done.\u001b[K\n",
            "remote: Counting objects: 100% (366/366), done.\u001b[K\n",
            "remote: Compressing objects: 100% (191/191), done.\u001b[K\n",
            "remote: Total 1124 (delta 261), reused 222 (delta 175), pack-reused 758 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1124/1124), 71.33 MiB | 19.88 MiB/s, done.\n",
            "Resolving deltas: 100% (622/622), done.\n",
            "Updating files: 100% (99/99), done.\n",
            "Already up to date.\n",
            "Branch 'sophie' set up to track remote branch 'sophie' from 'origin'.\n",
            "Switched to a new branch 'sophie'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b514b34b",
      "metadata": {
        "id": "b514b34b"
      },
      "source": [
        "## Load and preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Project data\n",
        "data_dir = os.path.join(content_base, \"My Drive/Capstone/data\")\n",
        "data_file = os.path.join(data_dir, \"subset_transactions2.csv\")"
      ],
      "metadata": {
        "id": "ku44RRP5oR9c"
      },
      "id": "ku44RRP5oR9c",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "716b394f",
      "metadata": {
        "id": "716b394f"
      },
      "outputs": [],
      "source": [
        "pl = GNNPipeline(data_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "35981a90",
      "metadata": {
        "id": "35981a90"
      },
      "outputs": [],
      "source": [
        "pl.rename_columns()\n",
        "pl.drop_duplicates()\n",
        "pl.check_for_null()\n",
        "pl.extract_currency_features()\n",
        "pl.extract_time_features()\n",
        "pl.create_unique_ids()\n",
        "pl.extract_additional_time_features()\n",
        "pl.cyclical_encoding()\n",
        "pl.apply_one_hot_encoding()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee2528a9",
      "metadata": {
        "id": "ee2528a9"
      },
      "source": [
        "## Split data into train/val/test, and continue with split-specific feature engineering\n",
        "There are some features that, if engineered or standardized using the whole dataset, could result in data leakage between our train/val/test sets. Therefore, we must split the data prior to these calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ed69d9c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed69d9c3",
        "outputId": "290cc9c4-862d-474e-8d4d-7db66d1c45e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    def split_train_test_val(self, X_cols=None, y_col=\"is_laundering\", test_size=0.15, val_size=0.15, split_type=\"temporal_agg\"):\n",
            "            return super().split_train_test_val(X_cols, y_col, test_size, val_size, split_type)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import inspect\n",
        "# Check default split mehod for GNN\n",
        "print(inspect.getsource(pl.split_train_test_val))\n",
        "\n",
        "# Temporal split for edges\n",
        "pl.split_train_test_val()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86b92f01",
      "metadata": {
        "id": "86b92f01"
      },
      "source": [
        "### Create node features\n",
        "Node features are specific to accounts, and include graph based features like pagerank and degree centrality, as well as some aggregate statistics such as net flow (total amount sent-total amount received for a specific account)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "381f8ff7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "381f8ff7",
        "outputId": "2a0ec26f-9c92-4241-b251-f16e3e74b03a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Computed node features for train with 107090 nodes.\n",
            "✅ Computed node features for val with 107355 nodes.\n",
            "✅ Computed node features for test with 107583 nodes.\n",
            "Index(['node_id', 'degree_centrality', 'pagerank', 'net_flow', 'avg_txn_out',\n",
            "       'avg_txn_in', 'std_txn_out', 'std_txn_in', 'num_unique_out_partners',\n",
            "       'num_unique_in_partners'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Compute node features split-specifically\n",
        "pl.compute_split_specific_node_features()\n",
        "\n",
        "# Scale only relevant node features (others like pagerank left raw)\n",
        "pl.scale_node_data_frames()\n",
        "\n",
        "print(pl.train_nodes.columns) # print node features to peek"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cdfbfe2",
      "metadata": {
        "id": "7cdfbfe2"
      },
      "source": [
        "### Create graph objects (GNN specific processes)\n",
        "The `split_train_test_val_graph()` function uses the data split above, and creates PyG-style Data objects. PyG-style Data objects have features like:\n",
        "\n",
        "- x: node (account) features (without column for \"node_id\", mind you--so these must be properly sorted and align with our unique edge indexers)\n",
        "- edge_index: a [2, num_transactions] tensor containing the accounts involved in each transaction\n",
        "- edge_attr: the edge (transaction) features, listed above, including things like amount, temporal features, and payment type\n",
        "- y: our labels -- 'is_laundering' column, associated with each transaction\n",
        "\n",
        "Another feature of our `split_train_test_val_graph` function is reordering columns such that we have 'edge_id' as the first column -- this is important for how our model works, since we use edge_id to determine which transactions to evaluate during model training, but then drop the column before passing the transactions into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "68bcb63d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68bcb63d",
        "outputId": "0e2b993c-8045-44ad-86a1-a18fc717768d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['day_cos', 'day_sin', 'edge_id', 'hour_of_day', 'is_weekend', 'log_exchange_rate', 'payment_type_ACH', 'payment_type_Bitcoin', 'payment_type_Cash', 'payment_type_Cheque', 'payment_type_Credit Card', 'payment_type_Reinvestment', 'payment_type_Wire', 'received_amount', 'received_currency_Australian Dollar', 'received_currency_Bitcoin', 'received_currency_Brazil Real', 'received_currency_Canadian Dollar', 'received_currency_Euro', 'received_currency_Mexican Peso', 'received_currency_Ruble', 'received_currency_Rupee', 'received_currency_Saudi Riyal', 'received_currency_Shekel', 'received_currency_Swiss Franc', 'received_currency_UK Pound', 'received_currency_US Dollar', 'received_currency_Yen', 'received_currency_Yuan', 'sent_amount', 'sent_amount_usd', 'sent_currency_Australian Dollar', 'sent_currency_Bitcoin', 'sent_currency_Brazil Real', 'sent_currency_Canadian Dollar', 'sent_currency_Euro', 'sent_currency_Mexican Peso', 'sent_currency_Ruble', 'sent_currency_Rupee', 'sent_currency_Saudi Riyal', 'sent_currency_Shekel', 'sent_currency_Swiss Franc', 'sent_currency_UK Pound', 'sent_currency_US Dollar', 'sent_currency_Yen', 'sent_currency_Yuan', 'time_diff_from', 'time_diff_to', 'time_of_day_cos', 'time_of_day_sin', 'timestamp_int', 'timestamp_scaled', 'turnaround_time']\n",
            "['edge_id', 'received_currency_Rupee', 'received_currency_Bitcoin', 'time_diff_from', 'payment_type_Bitcoin', 'timestamp_scaled', 'day_cos', 'sent_currency_Euro', 'sent_currency_Shekel', 'payment_type_Credit Card', 'sent_currency_Saudi Riyal', 'received_currency_Brazil Real', 'sent_currency_Bitcoin', 'sent_currency_Brazil Real', 'time_of_day_cos', 'payment_type_Cheque', 'sent_currency_Canadian Dollar', 'turnaround_time', 'sent_currency_Yuan', 'sent_currency_Yen', 'received_currency_Canadian Dollar', 'payment_type_Cash', 'received_currency_Euro', 'sent_currency_UK Pound', 'received_currency_Yen', 'sent_currency_Mexican Peso', 'sent_currency_Australian Dollar', 'payment_type_Reinvestment', 'sent_currency_Ruble', 'received_currency_Yuan', 'received_currency_US Dollar', 'received_currency_Ruble', 'sent_currency_US Dollar', 'received_currency_Mexican Peso', 'payment_type_Wire', 'received_currency_Swiss Franc', 'day_sin', 'received_currency_UK Pound', 'received_currency_Australian Dollar', 'payment_type_ACH', 'sent_amount_usd', 'sent_currency_Swiss Franc', 'time_diff_to', 'time_of_day_sin', 'received_currency_Shekel', 'received_currency_Saudi Riyal', 'sent_currency_Rupee', 'log_exchange_rate']\n"
          ]
        }
      ],
      "source": [
        "print(pl.X_cols)\n",
        "edge_feats = ['edge_id'] + list(set(pl.X_cols) - set(['timestamp_int','hour_of_day','is_weekend','edge_id','sent_amount','received_amount']))\n",
        "print(edge_feats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0dd271cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dd271cb",
        "outputId": "cbac59f2-61b3-4f7a-b232-69c76e35a434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['edge_id', 'received_currency_Rupee', 'received_currency_Bitcoin', 'time_diff_from', 'payment_type_Bitcoin', 'timestamp_scaled', 'day_cos', 'sent_currency_Euro', 'sent_currency_Shekel', 'payment_type_Credit Card', 'sent_currency_Saudi Riyal', 'received_currency_Brazil Real', 'sent_currency_Bitcoin', 'sent_currency_Brazil Real', 'time_of_day_cos', 'payment_type_Cheque', 'sent_currency_Canadian Dollar', 'turnaround_time', 'sent_currency_Yuan', 'sent_currency_Yen', 'received_currency_Canadian Dollar', 'payment_type_Cash', 'received_currency_Euro', 'sent_currency_UK Pound', 'received_currency_Yen', 'sent_currency_Mexican Peso', 'sent_currency_Australian Dollar', 'payment_type_Reinvestment', 'sent_currency_Ruble', 'received_currency_Yuan', 'received_currency_US Dollar', 'received_currency_Ruble', 'sent_currency_US Dollar', 'received_currency_Mexican Peso', 'payment_type_Wire', 'received_currency_Swiss Franc', 'day_sin', 'received_currency_UK Pound', 'received_currency_Australian Dollar', 'payment_type_ACH', 'sent_amount_usd', 'sent_currency_Swiss Franc', 'time_diff_to', 'time_of_day_sin', 'received_currency_Shekel', 'received_currency_Saudi Riyal', 'sent_currency_Rupee', 'log_exchange_rate']\n"
          ]
        }
      ],
      "source": [
        "# Convert into PyG-style Data objects\n",
        "pl.split_train_test_val_graph(edge_features=edge_feats)\n",
        "print(pl.edge_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "74d843a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74d843a6",
        "outputId": "788944a4-bb1a-4da6-8f76-ab50c1b195c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sent_amount_usd', 'timestamp_scaled', 'time_diff_from', 'time_diff_to', 'turnaround_time']\n"
          ]
        }
      ],
      "source": [
        "pl.scale_edge_features(edge_features_to_scale=['sent_amount_usd','timestamp_scaled','time_diff_from','time_diff_to','turnaround_time'])\n",
        "print(pl.scaled_edge_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c7eeeb93",
      "metadata": {
        "id": "c7eeeb93"
      },
      "outputs": [],
      "source": [
        "# Prepare data loaders for training\n",
        "pl.get_data_loaders()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "54077770",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54077770",
        "outputId": "e59fab9c-0d4e-43fc-dd5b-b5822f1b7cae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    def initialize_training(\n",
            "        self,\n",
            "        threshold: float=0.5,\n",
            "        epochs: int=50,\n",
            "        patience: int=10,\n",
            "    ) -> None:\n",
            "        \"\"\"Setup the model pipeline for training: metrics, model,\n",
            "        optimizer, scheduler, and criterion\n",
            "        \"\"\"\n",
            "        self.threshold = threshold\n",
            "        self.epochs = epochs\n",
            "        self.patience = patience\n",
            "        \n",
            "        \n",
            "\n",
            "        # Since `initialize_training` is run after preprocessing is\n",
            "        # done, we can define the node and edge features here. This\n",
            "        # does assume that column ordering between data frames and\n",
            "        # tensors is preserved, and it removes node and edge id\n",
            "        # TODO: ran into issue with this line bc node_id has already been dropped\n",
            "        if 'node_id' in self.nodes.columns:\n",
            "            self.node_feature_labels = self.nodes.drop(columns=\"node_id\").columns\n",
            "        else:\n",
            "            self.node_feature_labels = self.nodes.columns\n",
            "        self.edge_feature_labels = self.df[self.edge_features].drop(columns=\"edge_id\").columns\n",
            "\n",
            "\n",
            "\n",
            "        # Model setup\n",
            "        num_edge_features = self.train_data.edge_attr.shape[1]-1  # num edge feats - edge_id\n",
            "        num_node_features = self.train_data.x.shape[1]\n",
            "        self.model = GINe(n_node_feats=num_node_features, n_edge_feats=num_edge_features).to(self.device)\n",
            "        self.trainer = GNNTrainer(self.model, self)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(inspect.getsource(pl.initialize_training))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4a4f6451",
      "metadata": {
        "id": "4a4f6451"
      },
      "outputs": [],
      "source": [
        "pl.initialize_training(epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7b7f5fbc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b7f5fbc",
        "outputId": "6fac01ea-2230-4c2a-e315-0d58a02b61dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GINe(\n",
            "  (node_emb): Linear(in_features=9, out_features=100, bias=True)\n",
            "  (edge_emb): Linear(in_features=47, out_features=100, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-1): 2 x GINEConv(nn=Sequential(\n",
            "      (0): Linear(in_features=100, out_features=100, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=100, out_features=100, bias=True)\n",
            "    ))\n",
            "  )\n",
            "  (emlps): ModuleList(\n",
            "    (0-1): 2 x Sequential(\n",
            "      (0): Linear(in_features=300, out_features=100, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=100, out_features=100, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (batch_norms): ModuleList(\n",
            "    (0-1): 2 x BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (mlp): Sequential(\n",
            "    (0): Linear(in_features=300, out_features=50, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.10527690625126304, inplace=False)\n",
            "    (3): Linear(in_features=50, out_features=25, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.10527690625126304, inplace=False)\n",
            "    (6): Linear(in_features=25, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(pl.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1ee7889b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ee7889b",
        "outputId": "ccb70ebf-7182-47a7-ae00-104af53cca4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Training: 100%|██████████| 107/107 [00:30<00:00,  3.56it/s]\n"
          ]
        }
      ],
      "source": [
        "pl.trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pl.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "wUDTfeWIsU0G",
        "outputId": "588024c8-8853-42f4-a701-e93aace30644"
      },
      "id": "wUDTfeWIsU0G",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}