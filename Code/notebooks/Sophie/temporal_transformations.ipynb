{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from tabulate import tabulate\n",
    "from tqdm import trange\n",
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "from time import monotonic\n",
    "from pprint import pprint\n",
    "from google.colab import drive\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellTimer:\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "\n",
    "    def start(self, *args, **kwargs):\n",
    "        self.start_time = monotonic()\n",
    "\n",
    "    def stop(self, *args, **kwargs):\n",
    "        try:\n",
    "            delta = round(monotonic() - self.start_time, 2)\n",
    "            print(f\"\\n⏱️ Execution time: {delta}s\")\n",
    "        except TypeError:\n",
    "            # The `stop` will be called when the cell that\n",
    "            # defines `CellTimer` is executed, but `start`\n",
    "            # was never called, leading to a `TypeError` in\n",
    "            # the subtraction. Skip it\n",
    "            pass\n",
    "\n",
    "\n",
    "timer = CellTimer()\n",
    "ipython = get_ipython()\n",
    "ipython.events.register(\"pre_run_cell\", timer.start)\n",
    "ipython.events.register(\"post_run_cell\", timer.stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../HI-Small_Trans.csv\", parse_dates=[\"Timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename columns for clarity and standard formatting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(\n",
    "    columns={\n",
    "        \"Timestamp\": \"timestamp\",\n",
    "        \"From Bank\": \"from_bank\",\n",
    "        \"Account\": \"from_account\",\n",
    "        \"To Bank\": \"to_bank\",\n",
    "        \"Account.1\": \"to_account\",\n",
    "        \"Amount Received\": \"received_amount\",\n",
    "        \"Receiving Currency\": \"received_currency\",\n",
    "        \"Amount Paid\": \"sent_amount\",\n",
    "        \"Payment Currency\": \"sent_currency\",\n",
    "        \"Payment Format\": \"payment_type\",\n",
    "        \"Is Laundering\": \"is_laundering\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function for whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def df_label_encoder(df, columns):\n",
    "      le = preprocessing.LabelEncoder()\n",
    "      for i in columns:\n",
    "          df[i] = le.fit_transform(df[i].astype(str))\n",
    "      return df\n",
    "\n",
    "def preprocess(df):\n",
    "  \"\"\"\n",
    "  Preprocesses the entire dataframe, including node mappings, label encoding,\n",
    "  and time processing which is independent of data split.\n",
    "  \"\"\"\n",
    "  ## Create unique account - ID mapping ##\n",
    "  # Get unique account-bank combos (a couple of acct numbers found at multiple banks)\n",
    "  df['from_account_id'] = df['from_bank'].astype(str) + '_' + df['from_account'].astype(str)\n",
    "  df['to_account_id'] = df['to_bank'].astype(str) + '_' + df['to_account'].astype(str)\n",
    "\n",
    "  # Get list of unique account ids\n",
    "  df = df.reset_index(drop=True)\n",
    "  from_nodes = df[\"from_account_id\"].drop_duplicates().reset_index(drop=True)\n",
    "  to_nodes = df[\"to_account_id\"].drop_duplicates().reset_index(drop=True)\n",
    "  all_nodes = pd.concat([from_nodes, to_nodes]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "  # Map node identifiers to integer indices\n",
    "  node_mapping = {node: idx for idx, node in enumerate(all_nodes)}\n",
    "  df[\"from_account_idx\"] = df[\"from_account_id\"].map(node_mapping)\n",
    "  df[\"to_account_idx\"] = df[\"to_account_id\"].map(node_mapping)\n",
    "\n",
    "  ## Label encode categorical vars ##\n",
    "  # Use label encoding and let model learn (instead of one-hot embeddings)\n",
    "  df = df_label_encoder(df, [\"payment_type\", \"sent_currency\", \"received_currency\", \"from_bank\", \"to_bank\"])\n",
    "\n",
    "  ## Currency conversion ## (not using for now, just looking at temporal feats)\n",
    "  # usd_conversion = currency.get_usd_conversion(df)\n",
    "  # df['Amount Paid (USD)'] = df.apply(lambda row: row['Amount Paid'] * usd_conversion.get(row['Payment Currency'], 1), axis=1)\n",
    "  # df['Amount Received (USD)'] = df.apply(lambda row: row['Amount Received'] * usd_conversion.get(row['Receiving Currency'], 1), axis=1)\n",
    "\n",
    "  ## Time transformations ##\n",
    "  # Extract items from timestamp\n",
    "  df[\"time_of_day\"] = df[\"timestamp\"].dt.time\n",
    "  df[\"hour_of_day\"] = df[\"timestamp\"].dt.hour\n",
    "  df[\"day_of_week\"] = df[\"timestamp\"].dt.weekday # 0=Monday,...,6=Sunday\n",
    "  df[\"seconds_since_midnight\"] = (\n",
    "    df[\"timestamp\"].dt.hour * 3600 +  # Convert hours to seconds\n",
    "    df[\"timestamp\"].dt.minute * 60 +  # Convert minutes to seconds\n",
    "    df[\"timestamp\"].dt.second         # Keep seconds\n",
    "  )\n",
    "\n",
    "  # Transform timestamp to raw int unix\n",
    "  df[\"timestamp_int\"] = df[\"timestamp\"].astype(int) / 10**9\n",
    "\n",
    "  # Apply cyclical encoding\n",
    "  df[\"day_sin\"] = np.sin(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "  df[\"day_cos\"] = np.cos(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "  df[\"time_of_day_sin\"] = np.sin(2 * np.pi * df[\"seconds_since_midnight\"] / 86400)\n",
    "  df[\"time_of_day_cos\"] = np.cos(2 * np.pi * df[\"seconds_since_midnight\"] / 86400)\n",
    "\n",
    "  # Create binary weekend indicator\n",
    "  df[\"is_weekend\"] = df[\"day_of_week\"].isin([5, 6]).astype(int)\n",
    "\n",
    "  df.drop(columns=[\"from_account\",\"to_account\"], inplace=True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function for train-val-test separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_split(df, scaler_time, scaler_amount):\n",
    "    \"\"\"\n",
    "    Normalizes timestamp and transaction amounts using given scalers.\n",
    "    \"\"\"\n",
    "    # Sort transactions by time\n",
    "    df = df.sort_values(by=[\"from_account_idx\", \"timestamp\"])\n",
    "\n",
    "    # Apply scaling\n",
    "    df[\"timestamp_scaled\"] = scaler_time.transform(df[[\"timestamp_int\"]])\n",
    "    # df[[\"sent_amount_scaled\", \"received_amount_scaled\"]] = scaler_amount.transform(df[[\"sent_amount\", \"received_amount\"]])\n",
    "\n",
    "    # Compute time difference between transactions per account and convert to seconds\n",
    "    df[\"time_diff_from_acct\"] = df.groupby(\"from_account_idx\")[\"timestamp\"].diff().dt.total_seconds().fillna(0)\n",
    "    df[\"time_diff_to_acct\"] = df.groupby(\"to_account_idx\")[\"timestamp\"].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess entire df\n",
    "df_original = df.copy()\n",
    "df_transformed = preprocess(df_original)\n",
    "\n",
    "# Split train and test data using random stratification\n",
    "train_df_rs, test_df_rs = train_test_split(\n",
    "    df_transformed,\n",
    "    test_size=0.2,\n",
    "    stratify=df_transformed[\"is_laundering\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "### Random stratified\n",
    "### Scaling the data\n",
    "# Choose a scaler (MinMax or StandardScaler)\n",
    "scaler1 = MinMaxScaler() # Choose a scaler (MinMax or StandardScaler)\n",
    "scaler2 = StandardScaler()\n",
    "# Fit scalar to training data before preprocessing\n",
    "# Fit only on training data, then transform train/val/test separately\n",
    "# This makes scaling consistent (not confusing model)\n",
    "scaler1.fit(train_df_rs[[\"timestamp_int\"]])\n",
    "scaler2.fit(train_df_rs[[\"sent_amount\", \"received_amount\"]])  # Fit only on training data\n",
    "### Preprocess\n",
    "train_df_rs, test_df_rs = normalize(train_df_rs, scaler1, scaler2), normalize(test_df_rs, scaler1, scaler2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranformations for model pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applied to whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine bank + account for unique IDs as there were a few duplicate account numbers at different banks in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create unique account - ID mapping ##\n",
    "# Get unique account-bank combos (a couple of acct numbers found at multiple banks)\n",
    "df['from_account_id'] = df['from_bank'].astype(str) + '_' + df['from_account'].astype(str)\n",
    "df['to_account_id'] = df['to_bank'].astype(str) + '_' + df['to_account'].astype(str)\n",
    "df.drop(columns=[\"from_account\",\"to_account\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map new node ids (from_bank + from_account, to_bank + to_account) to integer indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of unique account ids\n",
    "df = df.reset_index(drop=True)\n",
    "from_nodes = df[\"from_account_id\"].drop_duplicates().reset_index(drop=True)\n",
    "to_nodes = df[\"to_account_id\"].drop_duplicates().reset_index(drop=True)\n",
    "all_nodes = pd.concat([from_nodes, to_nodes]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Map node identifiers to integer indices\n",
    "node_mapping = {node: idx for idx, node in enumerate(all_nodes)}\n",
    "df[\"from_account_idx\"] = df[\"from_account_id\"].map(node_mapping)\n",
    "df[\"to_account_idx\"] = df[\"to_account_id\"].map(node_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract items from timestamp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract items from timestamp\n",
    "df[\"hour_of_day\"] = df[\"timestamp\"].dt.hour\n",
    "df[\"day_of_week\"] = df[\"timestamp\"].dt.weekday # 0=Monday,...,6=Sunday\n",
    "df[\"seconds_since_midnight\"] = (\n",
    "df[\"timestamp\"].dt.hour * 3600 +  # Convert hours to seconds\n",
    "df[\"timestamp\"].dt.minute * 60 +  # Convert minutes to seconds\n",
    "df[\"timestamp\"].dt.second         # Keep seconds\n",
    ")\n",
    "# Transform timestamp to raw int unix\n",
    "df[\"timestamp_int\"] = df[\"timestamp\"].astype(int) / 10**9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cyclincal encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cyclical encoding\n",
    "df[\"day_sin\"] = np.sin(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "df[\"day_cos\"] = np.cos(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "df[\"time_of_day_sin\"] = np.sin(2 * np.pi * df[\"seconds_since_midnight\"] / 86400)\n",
    "df[\"time_of_day_cos\"] = np.cos(2 * np.pi * df[\"seconds_since_midnight\"] / 86400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create binary weekend indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_weekend\"] = df[\"day_of_week\"].isin([5, 6]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applied on train/val/test separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_split(df, scaler_time, scaler_amount):\n",
    "    \"\"\"\n",
    "    Normalizes timestamp and transaction amounts using given scalers.\n",
    "    \"\"\"\n",
    "    # Sort transactions by time\n",
    "    df = df.sort_values(by=[\"from_account_idx\", \"timestamp\"])\n",
    "\n",
    "    # Apply scaling\n",
    "    df[\"timestamp_scaled\"] = scaler_time.transform(df[[\"timestamp_int\"]])\n",
    "    \n",
    "    # Compute time difference between transactions per account and convert to seconds\n",
    "    df[\"time_diff_from_acct\"] = df.groupby(\"from_account_idx\")[\"timestamp\"].diff().dt.total_seconds().fillna(0)\n",
    "    df[\"time_diff_to_acct\"] = df.groupby(\"to_account_idx\")[\"timestamp\"].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Abhitay's pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1676916179.py, line 191)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 191\u001b[1;36m\u001b[0m\n\u001b[1;33m    self.scaler_time.fit(self.X_train[[\"timestamp\"]])\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class model_pipeline:\n",
    "\n",
    "    def __init__(self, df_path, random_state_):\n",
    "\n",
    "        self.df_path = df_path\n",
    "        self.df = pd.read_csv(self.df_path)\n",
    "        self.random_state_ = random_state_\n",
    "        \n",
    "        # Track if preprocessing steps have been completed\n",
    "        self.preprocessed = {\n",
    "            \"renamed\": False,\n",
    "            \"duplicates_removed\": False,\n",
    "            \"unique_ids_created\": False,\n",
    "            \"currency_normalized\": False,\n",
    "            \"time_features_extracted\": False,\n",
    "            \"cyclical_encoded\": False,\n",
    "            \"weekend_encoded\": False,\n",
    "            \"date_converted\": False,\n",
    "            \"features_encoded\": False,\n",
    "            \"neighbor_context_computed\": False,\n",
    "            \"normalized\": False\n",
    "        }\n",
    "        \n",
    "        # For ML pipeline\n",
    "\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "\n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "\n",
    "        self.X_val = None\n",
    "        self.y_val = None\n",
    "\n",
    "        self.y_pred = None\n",
    "        self.y_proba = None\n",
    "\n",
    "        self.model = None\n",
    "    \n",
    "    ### Data tidying ###\n",
    "    \n",
    "    def rename_columns(self):\n",
    "        \"\"\"Renames the columns of self.df to standardized names.\"\"\"\n",
    "        column_mapping = {\n",
    "            \"Timestamp\": \"timestamp\",\n",
    "            \"From Bank\": \"from_bank\",\n",
    "            \"Account\": \"from_account\",\n",
    "            \"To Bank\": \"to_bank\",\n",
    "            \"Account.1\": \"to_account\",\n",
    "            \"Amount Received\": \"received_amount\",\n",
    "            \"Receiving Currency\": \"received_currency\",\n",
    "            \"Amount Paid\": \"sent_amount\",\n",
    "            \"Payment Currency\": \"sent_currency\",\n",
    "            \"Payment Format\": \"payment_type\",\n",
    "            \"Is Laundering\": \"is_laundering\",\n",
    "        }\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        missing_columns = [col for col in column_mapping.keys() if col not in self.df.columns]\n",
    "        if missing_columns:\n",
    "            raise KeyError(f\"Missing expected columns in dataset: {missing_columns}\")\n",
    "\n",
    "        self.df.rename(columns=column_mapping, inplace=True)\n",
    "        self.preprocessed[\"renamed\"] = True\n",
    "        \n",
    "    def drop_duplicates(self):\n",
    "        self.df.drop_duplicates(inplace=True)\n",
    "        self.preprocessed[\"duplicates_removed\"] = True\n",
    "        \n",
    "    def create_unique_ids(self):\n",
    "        \"\"\"Create unique account - ID mapping.\"\"\"\n",
    "        if not self.preprocessed[\"renamed\"]:\n",
    "            raise RuntimeError(\"Columns must be renamed (run rename()) before creating unique IDs.\")\n",
    "\n",
    "        # Get unique account-bank combos (a couple of acct numbers found at multiple banks)\n",
    "        self.df['from_account_id'] = self.df['from_bank'].astype(str) + '_' + self.df['from_account'].astype(str)\n",
    "        self.df['to_account_id'] = self.df['to_bank'].astype(str) + '_' + self.df['to_account'].astype(str)\n",
    "        self.df.drop(columns=[\"from_account\",\"to_account\"], inplace=True)\n",
    "        \n",
    "        # Get list of unique account ids\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        from_nodes = self.df[\"from_account_id\"].drop_duplicates().reset_index(drop=True)\n",
    "        to_nodes = self.df[\"to_account_id\"].drop_duplicates().reset_index(drop=True)\n",
    "        all_nodes = pd.concat([from_nodes, to_nodes]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "        # Map node identifiers to integer indices\n",
    "        node_mapping = {node: idx for idx, node in enumerate(all_nodes)}\n",
    "        df[\"from_account_idx\"] = df[\"from_account_id\"].map(node_mapping)\n",
    "        df[\"to_account_idx\"] = df[\"to_account_id\"].map(node_mapping)\n",
    "        \n",
    "        self.preprocessed[\"unique_ids_created\"] = True\n",
    "        \n",
    "    ### Summary statistics ###\n",
    "       \n",
    "    def df_summary(self):\n",
    "        print('DATA HEAD')\n",
    "        display(self.df.head())\n",
    "        print('FEATURE TYPE')\n",
    "        display(self.df.info())\n",
    "\n",
    "    def y_statistics(self):\n",
    "        print('Normalised Value Count: ')\n",
    "        print(self.df['is_laundering'].value_counts(normalize=True))\n",
    "\n",
    "    ### Feature Transformation ###\n",
    "\n",
    "    def currency_normalization(self):\n",
    "        if \"sent_currency\" not in self.df.columns or \"received_currency\" not in self.df.columns:\n",
    "            raise KeyError(\"Currency columns missing. Need to run rename() so in proper format\")\n",
    "        \n",
    "        usd_conversion = currency.get_usd_conversion(self.df_path)\n",
    "        self.df['sent_amount_USD'] = self.df.apply(lambda row: row['sent_amount'] * usd_conversion.get(row['sent_currency'], 1), axis=1)\n",
    "        self.df['received_amount_USD'] = self.df.apply(lambda row: row['received_amount'] * usd_conversion.get(row['received_currency'], 1), axis=1)\n",
    "\n",
    "    def extract_time_features(self):\n",
    "        if \"timestamp\" not in self.df.columns:\n",
    "            raise KeyError(\"Missing 'timestamp' column, run rename().\")\n",
    "        if not isinstance(self.df[\"timestamp\"], datetime.datetime):\n",
    "            self.df[\"timestamp\"] = pd.to_datetime(self.df[\"timestamp\"])\n",
    "        \n",
    "        # Extract items from timestamp\n",
    "        self.df[\"hour_of_day\"] = self.df[\"timestamp\"].dt.hour\n",
    "        self.df[\"day_of_week\"] = self.df[\"timestamp\"].dt.weekday # 0=Monday,...,6=Sunday\n",
    "        self.df[\"seconds_since_midnight\"] = (\n",
    "            self.df[\"timestamp\"].dt.hour * 3600 +  # Convert hours to seconds\n",
    "            self.df[\"timestamp\"].dt.minute * 60 +  # Convert minutes to seconds\n",
    "            self.df[\"timestamp\"].dt.second         # Keep seconds\n",
    "        )\n",
    "        \n",
    "        self.preprocessed[\"time_features_extracted\"] = True\n",
    "        \n",
    "    def cyclical_encoding(self):\n",
    "        if not self.preprocessed[\"time_features_extracted\"]:\n",
    "            raise RuntimeError(\"Time features missing, run `extract_time_features` first.\")\n",
    "\n",
    "        self.df[\"day_sin\"] = np.sin(2 * np.pi * self.df[\"day_of_week\"] / 7)\n",
    "        self.df[\"day_cos\"] = np.cos(2 * np.pi * self.df[\"day_of_week\"] / 7)\n",
    "        self.df[\"time_of_day_sin\"] = np.sin(2 * np.pi * self.df[\"seconds_since_midnight\"] / 86400)\n",
    "        self.df[\"time_of_day_cos\"] = np.cos(2 * np.pi * self.df[\"seconds_since_midnight\"] / 86400)\n",
    "        \n",
    "        self.preprocessed[\"cyclical_encoded\"] = True\n",
    "        \n",
    "    def binary_weekend(self):\n",
    "        if \"day_of_week\" not in self.df.columns:\n",
    "            raise KeyError(\"Day-of-week feature missing. Run `extract_time_features` first.\")\n",
    "        self.df[\"is_weekend\"] = self.df[\"day_of_week\"].isin([5, 6]).astype(int)\n",
    "        self.preprocessed[\"weekend_encoded\"] = True\n",
    "    \n",
    "    def date_to_unix(self):\n",
    "        \"\"\"Converts timestamp to Unix time.\"\"\"\n",
    "        if \"timestamp\" not in self.df.columns:\n",
    "            raise KeyError(\"Missing 'timestamp' column.\")\n",
    "        self.df[\"timestamp\"] = self.df[\"timestamp\"].astype(int) / 10**9\n",
    "        self.preprocessed[\"date_converted\"] = True\n",
    "        \n",
    "    def label_encoding(self, features_to_encode):\n",
    "        if not self.preprocessed[\"renamed\"]:\n",
    "            raise RuntimeError(\"Run rename() before encoding categorical features.\")\n",
    "        \n",
    "        for col in features_to_encode:\n",
    "            if col not in self.df.columns:\n",
    "                raise KeyError(f\"Column '{col}' not found in the dataset.\")\n",
    "            self.df[col] = LabelEncoder().fit_transform(self.df[col])\n",
    "        \n",
    "        self.preprocessed[\"features_encoded\"] = True\n",
    "\n",
    "    def neighbor_context(self):\n",
    "        if not self.preprocessed[\"unique_ids_created\"]:\n",
    "            raise RuntimeError(\"Unique account IDs must be created before computing network features.\")\n",
    "        \n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        for _, row in self.df.iterrows():\n",
    "            G.add_edge(row['from_account_idx'], row['to_account_idx'], weight=row['sent_amount_USD'])\n",
    "\n",
    "        self.df['degree_centrality'] = self.df['from_account_idx'].map(nx.degree_centrality(G))\n",
    "        self.df['pagerank'] = self.df['from_account_idx'].map(nx.pagerank(G))\n",
    "        \n",
    "        self.preprocessed[\"neighbor_context_computed\"] = True\n",
    "        \n",
    "    ## Applied to train and test separately\n",
    "    def normalize_train_test(self):\n",
    "        \"\"\"Normalizes timestamp and transaction amounts separately for train and test sets.\"\"\"\n",
    "        if not self.preprocessed[\"date_converted\"]:\n",
    "            raise RuntimeError(\"Timestamps must be converted before normalization.\")\n",
    "        if not self.preprocessed[\"currency_normalized\"]:\n",
    "            raise RuntimeError(\"Convert currency to USD first\")\n",
    "        if self.X_train is None:\n",
    "            raise RuntimeError(\"Split data into train and test first\")\n",
    "        \n",
    "        # Fit scalers only on training data\n",
    "        self.scaler_time.fit(self.X_train[[\"timestamp\"]])\n",
    "        self.scaler_amount.fit(self.X_train[[\"sent_amount_USD\", \"received_amount_USD\"]])\n",
    "        \n",
    "        # Apply scaling separately on train and test data\n",
    "        self.X_train[\"timestamp_scaled\"] = self.scaler_time.transform(self.X_train[[\"timestamp\"]])\n",
    "        self.X_test[\"timestamp_scaled\"] = self.scaler_time.transform(self.X_test[[\"timestamp\"]])\n",
    "        \n",
    "        self.X_train[[\"sent_amount_USD_scaled\", \"received_amount_USD_scaled\"]] = self.scaler_amount.transform(\n",
    "            self.X_train[[\"sent_amount_USD\", \"received_amount_USD\"]]\n",
    "        )\n",
    "        self.X_test[[\"sent_amount_USD_scaled\", \"received_amount_USD_scaled\"]] = self.scaler_amount.transform(\n",
    "            self.X_test[[\"sent_amount_USD\", \"received_amount_USD\"]]\n",
    "        )\n",
    "        \n",
    "        # Compute time differences separately for train and test\n",
    "        self.X_train[\"time_diff_from_acct\"] = self.X_train.groupby(\"from_account_idx\")[\"timestamp\"].diff().dt.total_seconds().fillna(0)\n",
    "        self.X_train[\"time_diff_to_acct\"] = self.X_train.groupby(\"to_account_idx\")[\"timestamp\"].diff().dt.total_seconds().fillna(0)\n",
    "        \n",
    "        self.X_test[\"time_diff_from_acct\"] = self.X_test.groupby(\"from_account_idx\")[\"timestamp\"].diff().dt.total_seconds().fillna(0)\n",
    "        self.X_test[\"time_diff_to_acct\"] = self.X_test.groupby(\"to_account_idx\")[\"timestamp\"].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "        self.preprocessed[\"normalized\"] = True\n",
    "        \n",
    "    ### Preprocessing Pipeline ###\n",
    "    \n",
    "    def run_preprocessing(self):\n",
    "        \"\"\"Runs all preprocessing steps in the correct order.\"\"\"\n",
    "        print(\"Running preprocessing pipeline...\")\n",
    "        \n",
    "        try:\n",
    "            self.rename_columns()\n",
    "            self.drop_duplicates()\n",
    "            self.create_unique_ids()\n",
    "            self.currency_normalization()\n",
    "            self.extract_time_features()\n",
    "            self.cyclical_encoding()\n",
    "            self.binary_weekend()\n",
    "            self.date_to_unix()\n",
    "            self.label_encoding()\n",
    "            self.neighbor_context()\n",
    "            print(\"Preprocessing completed successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in preprocessing: {e}\")\n",
    "\n",
    "    def generate_tensor(self,edge_features):\n",
    "        self.train_node_features = torch.tensor(self.X_train[edge_features].values, dtype=torch.float)\n",
    "        labels = torch.tensor(self.y_train.values, dtype=torch.long)\n",
    "        edge_index = torch.tensor(self.X_train[['from_account_idx', 'to_account_idx']].values.T, dtype=torch.long)\n",
    "        self.train_data = Data(x=self.train_node_features, edge_index=edge_index, y=labels)\n",
    "\n",
    "        self.test_node_features = torch.tensor(self.X_test[edge_features].values, dtype=torch.float)\n",
    "        labels = torch.tensor(self.y_test.values, dtype=torch.long)\n",
    "        edge_index = torch.tensor(self.X_test[['from_account_idx', 'to_account_idx']].values.T, dtype=torch.long)\n",
    "        self.test_data = Data(x=self.test_node_features, edge_index=edge_index, y=labels)\n",
    "    \n",
    "    def split_x_y(self, X_cols, y_col):\n",
    "        self.X = self.df[X_cols]\n",
    "        self.y = self.df[y_col]\n",
    "\n",
    "    def split_train_test(self, test_size_):\n",
    "        # Random Split For now\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=test_size_, random_state=self.random_state_, stratify=self.y)\n",
    "        # print(self.X_train.shape)\n",
    "        # print(self.X_test.shape)\n",
    "        # print(self.y_train.shape)\n",
    "        # print(self.y_test.shape)\n",
    "    \n",
    "    def random_forest_classifier(self, param):      \n",
    "        self.model = RandomForestClassifier(**param)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "\n",
    "    def xgboost_classifier(self,param):\n",
    "        self.dtrain = xgb.DMatrix(self.X_train, label=self.y_train)\n",
    "        self.dtest = xgb.DMatrix(self.X_test, label=self.y_test)\n",
    "        self.model = xgb.train(param, self.dtrain)\n",
    "\n",
    "    def training_gnn_model(self, learning_rate, epoch_,gnn_model):\n",
    "            \n",
    "        self.model = globals()[gnn_model](input_dim=self.train_node_features.shape[1], hidden_dim=16, output_dim=2)\n",
    "        # Define optimizer and loss function\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(device)\n",
    "        self.train_data = self.train_data.to(device)\n",
    "\n",
    "        # Training loop\n",
    "        epochs = epoch_\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = self.model(self.train_data.x, self.train_data.edge_index)\n",
    "            loss = criterion(out, self.train_data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    def predict_model_gnn(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            out_probs = self.model(self.test_data.x, self.test_data.edge_index)\n",
    "            self.y_proba = out_probs.cpu().numpy()\n",
    "            self.y_pred = out_probs.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    def predict_model(self, xgboost_flag = 'null'):\n",
    "        if xgboost_flag == 'null':\n",
    "            self.y_pred = self.model.predict(self.X_test)\n",
    "            self.y_proba = self.model.predict_proba(self.X_test)\n",
    "        else:\n",
    "            self.y_proba = self.model.predict(self.dtest)\n",
    "            self.y_pred = (self.y_proba > 0.5).astype(int)\n",
    "\n",
    "    def result_metrics(self):\n",
    "\n",
    "        print(classification_report(self.y_test, self.y_pred, digits=4))\n",
    "\n",
    "\n",
    "        cm = confusion_matrix(self.y_test, self.y_pred)\n",
    "        accuracy = balanced_accuracy_score(self.y_test, self.y_pred) \n",
    "        mcc = matthews_corrcoef(self.y_test, self.y_pred)\n",
    "        logloss = log_loss(self.y_test, self.y_proba) if self.y_proba is not None else None\n",
    "\n",
    "        print(f\"Balanced Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n",
    "        if logloss:\n",
    "            print(f\"Log Loss: {logloss:.4f}\")\n",
    "\n",
    "\n",
    "        if self.y_proba is not None:\n",
    "\n",
    "            if len(self.y_proba.shape) == 1:\n",
    "                fpr, tpr, _ = roc_curve(self.y_test, self.y_proba)\n",
    "                roc_auc = roc_auc_score(self.y_test, self.y_proba)\n",
    "                precision, recall, _ = precision_recall_curve(self.y_test, self.y_proba)\n",
    "                pr_auc = auc(recall, precision)\n",
    "            else:\n",
    "                fpr, tpr, _ = roc_curve(self.y_test, self.y_proba[:, 1])\n",
    "                roc_auc = roc_auc_score(self.y_test, self.y_proba[:, 1])\n",
    "                precision, recall, _ = precision_recall_curve(self.y_test, self.y_proba[:, 1])\n",
    "                pr_auc = auc(recall, precision)\n",
    "\n",
    "            print(f\"AUC-ROC Score: {roc_auc:.4f}\")\n",
    "            print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "\n",
    "\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "            class_labels = [\"Licit\", \"Illicit\"] \n",
    "            sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[0])\n",
    "            axes[0].set_title(\"Confusion Matrix\")\n",
    "            axes[0].set_xlabel(\"Predicted Label\")\n",
    "            axes[0].set_ylabel(\"True Label\")\n",
    "            axes[0].set_xticklabels(class_labels)\n",
    "            axes[0].set_yticklabels(class_labels)\n",
    "\n",
    "            axes[1].plot(fpr, tpr, label=f'ROC AUC = {roc_auc:.4f}')\n",
    "            axes[1].plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")  # Baseline\n",
    "            axes[1].set_title(\"ROC Curve\")\n",
    "            axes[1].set_xlabel(\"False Positive Rate\")\n",
    "            axes[1].set_ylabel(\"True Positive Rate\")\n",
    "            axes[1].legend()\n",
    "\n",
    "\n",
    "            axes[2].plot(recall, precision, label=f'PR AUC = {pr_auc:.4f}')\n",
    "            axes[2].set_title(\"Precision-Recall Curve\")\n",
    "            axes[2].set_xlabel(\"Recall\")\n",
    "            axes[2].set_ylabel(\"Precision\")\n",
    "            axes[2].legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
