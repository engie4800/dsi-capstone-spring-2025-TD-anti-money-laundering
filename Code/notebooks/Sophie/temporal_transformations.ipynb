{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from tabulate import tabulate\n",
    "from tqdm import trange\n",
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "from time import monotonic\n",
    "from pprint import pprint\n",
    "from google.colab import drive\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Project Source Code\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"../../src\"))\n",
    "sys.path.append(src_path)\n",
    "from helpers import add_cell_timer\n",
    "\n",
    "add_cell_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../HI-Small_Trans.csv\", parse_dates=[\"Timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename columns for clarity and standard formatting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(\n",
    "    columns={\n",
    "        \"Timestamp\": \"timestamp\",\n",
    "        \"From Bank\": \"from_bank\",\n",
    "        \"Account\": \"from_account\",\n",
    "        \"To Bank\": \"to_bank\",\n",
    "        \"Account.1\": \"to_account\",\n",
    "        \"Amount Received\": \"received_amount\",\n",
    "        \"Receiving Currency\": \"received_currency\",\n",
    "        \"Amount Paid\": \"sent_amount\",\n",
    "        \"Payment Currency\": \"sent_currency\",\n",
    "        \"Payment Format\": \"payment_type\",\n",
    "        \"Is Laundering\": \"is_laundering\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function for whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def df_label_encoder(df, columns):\n",
    "      le = preprocessing.LabelEncoder()\n",
    "      for i in columns:\n",
    "          df[i] = le.fit_transform(df[i].astype(str))\n",
    "      return df\n",
    "\n",
    "def preprocess(df):\n",
    "  \"\"\"\n",
    "  Preprocesses the entire dataframe, including node mappings, label encoding,\n",
    "  and time processing which is independent of data split.\n",
    "  \"\"\"\n",
    "  ## Create unique account - ID mapping ##\n",
    "  # Get unique account-bank combos (a couple of acct numbers found at multiple banks)\n",
    "  df['from_account_id'] = df['from_bank'].astype(str) + '_' + df['from_account'].astype(str)\n",
    "  df['to_account_id'] = df['to_bank'].astype(str) + '_' + df['to_account'].astype(str)\n",
    "\n",
    "  # Get list of unique account ids\n",
    "  df = df.reset_index(drop=True)\n",
    "  from_nodes = df[\"from_account_id\"].drop_duplicates().reset_index(drop=True)\n",
    "  to_nodes = df[\"to_account_id\"].drop_duplicates().reset_index(drop=True)\n",
    "  all_nodes = pd.concat([from_nodes, to_nodes]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "  # Map node identifiers to integer indices\n",
    "  node_mapping = {node: idx for idx, node in enumerate(all_nodes)}\n",
    "  df[\"from_account_idx\"] = df[\"from_account_id\"].map(node_mapping)\n",
    "  df[\"to_account_idx\"] = df[\"to_account_id\"].map(node_mapping)\n",
    "\n",
    "  ## Label encode categorical vars ##\n",
    "  # Use label encoding and let model learn (instead of one-hot embeddings)\n",
    "  df = df_label_encoder(df, [\"payment_type\", \"sent_currency\", \"received_currency\", \"from_bank\", \"to_bank\"])\n",
    "\n",
    "  ## Currency conversion ## (not using for now, just looking at temporal feats)\n",
    "  # usd_conversion = currency.get_usd_conversion(df)\n",
    "  # df['Amount Paid (USD)'] = df.apply(lambda row: row['Amount Paid'] * usd_conversion.get(row['Payment Currency'], 1), axis=1)\n",
    "  # df['Amount Received (USD)'] = df.apply(lambda row: row['Amount Received'] * usd_conversion.get(row['Receiving Currency'], 1), axis=1)\n",
    "\n",
    "  ## Time transformations ##\n",
    "  # Extract items from timestamp\n",
    "  df[\"time_of_day\"] = df[\"timestamp\"].dt.time\n",
    "  df[\"hour_of_day\"] = df[\"timestamp\"].dt.hour\n",
    "  df[\"day_of_week\"] = df[\"timestamp\"].dt.weekday # 0=Monday,...,6=Sunday\n",
    "  df[\"seconds_since_midnight\"] = (\n",
    "    df[\"timestamp\"].dt.hour * 3600 +  # Convert hours to seconds\n",
    "    df[\"timestamp\"].dt.minute * 60 +  # Convert minutes to seconds\n",
    "    df[\"timestamp\"].dt.second         # Keep seconds\n",
    "  )\n",
    "\n",
    "  # Transform timestamp to raw int unix\n",
    "  df[\"timestamp_int\"] = df[\"timestamp\"].astype(int) / 10**9\n",
    "\n",
    "  # Apply cyclical encoding\n",
    "  df[\"day_sin\"] = np.sin(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "  df[\"day_cos\"] = np.cos(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "  df[\"time_of_day_sin\"] = np.sin(2 * np.pi * df[\"seconds_since_midnight\"] / 86400)\n",
    "  df[\"time_of_day_cos\"] = np.cos(2 * np.pi * df[\"seconds_since_midnight\"] / 86400)\n",
    "\n",
    "  # Create binary weekend indicator\n",
    "  df[\"is_weekend\"] = df[\"day_of_week\"].isin([5, 6]).astype(int)\n",
    "\n",
    "  df.drop(columns=[\"from_account\",\"to_account\"], inplace=True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function for train-val-test separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_split(df, scaler_time, scaler_amount):\n",
    "    \"\"\"\n",
    "    Normalizes timestamp and transaction amounts using given scalers.\n",
    "    \"\"\"\n",
    "    # Sort transactions by time\n",
    "    df = df.sort_values(by=[\"from_account_idx\", \"timestamp\"])\n",
    "\n",
    "    # Apply scaling\n",
    "    df[\"timestamp_scaled\"] = scaler_time.transform(df[[\"timestamp_int\"]])\n",
    "    # df[[\"sent_amount_scaled\", \"received_amount_scaled\"]] = scaler_amount.transform(df[[\"sent_amount\", \"received_amount\"]])\n",
    "\n",
    "    # Compute time difference between transactions per account and convert to seconds\n",
    "    df[\"time_diff_from_acct\"] = df.groupby(\"from_account_idx\")[\"timestamp\"].diff().dt.total_seconds().fillna(0)\n",
    "    df[\"time_diff_to_acct\"] = df.groupby(\"to_account_idx\")[\"timestamp\"].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess entire df\n",
    "df_original = df.copy()\n",
    "df_transformed = preprocess(df_original)\n",
    "\n",
    "# Split train and test data using random stratification\n",
    "train_df_rs, test_df_rs = train_test_split(\n",
    "    df_transformed,\n",
    "    test_size=0.2,\n",
    "    stratify=df_transformed[\"is_laundering\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "### Random stratified\n",
    "### Scaling the data\n",
    "# Choose a scaler (MinMax or StandardScaler)\n",
    "scaler1 = MinMaxScaler() # Choose a scaler (MinMax or StandardScaler)\n",
    "scaler2 = StandardScaler()\n",
    "# Fit scalar to training data before preprocessing\n",
    "# Fit only on training data, then transform train/val/test separately\n",
    "# This makes scaling consistent (not confusing model)\n",
    "scaler1.fit(train_df_rs[[\"timestamp_int\"]])\n",
    "scaler2.fit(train_df_rs[[\"sent_amount\", \"received_amount\"]])  # Fit only on training data\n",
    "### Preprocess\n",
    "train_df_rs, test_df_rs = preprocess_split(train_df_rs, scaler1, scaler2), preprocess_split(test_df_rs, scaler1, scaler2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranformations for model pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applied to whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine bank + account for unique IDs as there were a few duplicate account numbers at different banks in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create unique account - ID mapping ##\n",
    "# Get unique account-bank combos (a couple of acct numbers found at multiple banks)\n",
    "df['from_account_id'] = df['from_bank'].astype(str) + '_' + df['from_account'].astype(str)\n",
    "df['to_account_id'] = df['to_bank'].astype(str) + '_' + df['to_account'].astype(str)\n",
    "df.drop(columns=[\"from_account\",\"to_account\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map new node ids (from_bank + from_account, to_bank + to_account) to integer indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of unique account ids\n",
    "df = df.reset_index(drop=True)\n",
    "from_nodes = df[\"from_account_id\"].drop_duplicates().reset_index(drop=True)\n",
    "to_nodes = df[\"to_account_id\"].drop_duplicates().reset_index(drop=True)\n",
    "all_nodes = pd.concat([from_nodes, to_nodes]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Map node identifiers to integer indices\n",
    "node_mapping = {node: idx for idx, node in enumerate(all_nodes)}\n",
    "df[\"from_account_idx\"] = df[\"from_account_id\"].map(node_mapping)\n",
    "df[\"to_account_idx\"] = df[\"to_account_id\"].map(node_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract items from timestamp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract items from timestamp\n",
    "df[\"hour_of_day\"] = df[\"timestamp\"].dt.hour\n",
    "df[\"day_of_week\"] = df[\"timestamp\"].dt.weekday # 0=Monday,...,6=Sunday\n",
    "df[\"seconds_since_midnight\"] = (\n",
    "df[\"timestamp\"].dt.hour * 3600 +  # Convert hours to seconds\n",
    "df[\"timestamp\"].dt.minute * 60 +  # Convert minutes to seconds\n",
    "df[\"timestamp\"].dt.second         # Keep seconds\n",
    ")\n",
    "# Transform timestamp to raw int unix\n",
    "df[\"timestamp_int\"] = df[\"timestamp\"].astype(int) / 10**9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cyclincal encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cyclical encoding\n",
    "df[\"day_sin\"] = np.sin(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "df[\"day_cos\"] = np.cos(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "df[\"time_of_day_sin\"] = np.sin(2 * np.pi * df[\"seconds_since_midnight\"] / 86400)\n",
    "df[\"time_of_day_cos\"] = np.cos(2 * np.pi * df[\"seconds_since_midnight\"] / 86400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create binary weekend indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_weekend\"] = df[\"day_of_week\"].isin([5, 6]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applied on train/val/test separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_split(df, scaler_time, scaler_amount):\n",
    "    \"\"\"\n",
    "    Normalizes timestamp and transaction amounts using given scalers.\n",
    "    \"\"\"\n",
    "    # Sort transactions by time\n",
    "    df = df.sort_values(by=[\"from_account_idx\", \"timestamp\"])\n",
    "\n",
    "    # Apply scaling\n",
    "    df[\"timestamp_scaled\"] = scaler_time.transform(df[[\"timestamp_int\"]])\n",
    "    \n",
    "    # Compute time difference between transactions per account and convert to seconds\n",
    "    df[\"time_diff_from_acct\"] = df.groupby(\"from_account_idx\")[\"timestamp\"].diff().dt.total_seconds().fillna(0)\n",
    "    df[\"time_diff_to_acct\"] = df.groupby(\"to_account_idx\")[\"timestamp\"].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
